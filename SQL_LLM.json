{"id":"4cae43b5-4550-4d03-8001-db529510affb","data":{"nodes":[{"data":{"description":"Get chat inputs from the Playground.","display_name":"Chat Input","id":"ChatInput-jV0J9","node":{"base_classes":["Message"],"beta":false,"conditional_paths":[],"custom_fields":{},"description":"Get chat inputs from the Playground.","display_name":"Chat Input","documentation":"","edited":false,"field_order":["input_value","store_message","sender","sender_name","session_id","files"],"frozen":false,"icon":"ChatInput","metadata":{},"output_types":[],"outputs":[{"cache":true,"display_name":"Message","method":"message_response","name":"message","selected":"Message","types":["Message"],"value":"__UNDEFINED__"}],"pinned":false,"template":{"_type":"Component","code":{"advanced":true,"dynamic":true,"fileTypes":[],"file_path":"","info":"","list":false,"load_from_db":false,"multiline":true,"name":"code","password":false,"placeholder":"","required":true,"show":true,"title_case":false,"type":"code","value":"from langflow.base.data.utils import IMG_FILE_TYPES, TEXT_FILE_TYPES\nfrom langflow.base.io.chat import ChatComponent\nfrom langflow.inputs import BoolInput\nfrom langflow.io import DropdownInput, FileInput, MessageTextInput, MultilineInput, Output\nfrom langflow.memory import store_message\nfrom langflow.schema.message import Message\nfrom langflow.utils.constants import MESSAGE_SENDER_AI, MESSAGE_SENDER_NAME_USER, MESSAGE_SENDER_USER\n\n\nclass ChatInput(ChatComponent):\n    display_name = \"Chat Input\"\n    description = \"Get chat inputs from the Playground.\"\n    icon = \"ChatInput\"\n    name = \"ChatInput\"\n\n    inputs = [\n        MultilineInput(\n            name=\"input_value\",\n            display_name=\"Text\",\n            value=\"\",\n            info=\"Message to be passed as input.\",\n        ),\n        BoolInput(\n            name=\"should_store_message\",\n            display_name=\"Store Messages\",\n            info=\"Store the message in the history.\",\n            value=True,\n            advanced=True,\n        ),\n        DropdownInput(\n            name=\"sender\",\n            display_name=\"Sender Type\",\n            options=[MESSAGE_SENDER_AI, MESSAGE_SENDER_USER],\n            value=MESSAGE_SENDER_USER,\n            info=\"Type of sender.\",\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"sender_name\",\n            display_name=\"Sender Name\",\n            info=\"Name of the sender.\",\n            value=MESSAGE_SENDER_NAME_USER,\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"session_id\",\n            display_name=\"Session ID\",\n            info=\"The session ID of the chat. If empty, the current session ID parameter will be used.\",\n            advanced=True,\n        ),\n        FileInput(\n            name=\"files\",\n            display_name=\"Files\",\n            file_types=TEXT_FILE_TYPES + IMG_FILE_TYPES,\n            info=\"Files to be sent with the message.\",\n            advanced=True,\n            is_list=True,\n        ),\n    ]\n    outputs = [\n        Output(display_name=\"Message\", name=\"message\", method=\"message_response\"),\n    ]\n\n    def message_response(self) -> Message:\n        message = Message(\n            text=self.input_value,\n            sender=self.sender,\n            sender_name=self.sender_name,\n            session_id=self.session_id,\n            files=self.files,\n        )\n\n        if (\n            self.session_id\n            and isinstance(message, Message)\n            and isinstance(message.text, str)\n            and self.should_store_message\n        ):\n            store_message(\n                message,\n                flow_id=self.graph.flow_id,\n            )\n            self.message.value = message\n\n        self.status = message\n        return message\n"},"files":{"advanced":true,"display_name":"Files","dynamic":false,"fileTypes":["txt","md","mdx","csv","json","yaml","yml","xml","html","htm","pdf","docx","py","sh","sql","js","ts","tsx","jpg","jpeg","png","bmp","image"],"file_path":"","info":"Files to be sent with the message.","list":true,"name":"files","placeholder":"","required":false,"show":true,"title_case":false,"trace_as_metadata":true,"type":"file","value":""},"input_value":{"advanced":false,"display_name":"Text","dynamic":false,"info":"Message to be passed as input.","input_types":["Message"],"list":false,"load_from_db":false,"multiline":true,"name":"input_value","placeholder":"","required":false,"show":true,"title_case":false,"trace_as_input":true,"trace_as_metadata":true,"type":"str","value":"告訴我Email為tracy35@example.org的客戶最近一次購買的產品，包含產品名稱與購買日期。"},"sender":{"advanced":true,"display_name":"Sender Type","dynamic":false,"info":"Type of sender.","name":"sender","options":["Machine","User"],"placeholder":"","required":false,"show":true,"title_case":false,"trace_as_metadata":true,"type":"str","value":"User"},"sender_name":{"advanced":true,"display_name":"Sender Name","dynamic":false,"info":"Name of the sender.","input_types":["Message"],"list":false,"load_from_db":false,"name":"sender_name","placeholder":"","required":false,"show":true,"title_case":false,"trace_as_input":true,"trace_as_metadata":true,"type":"str","value":"User"},"session_id":{"advanced":true,"display_name":"Session ID","dynamic":false,"info":"The session ID of the chat. If empty, the current session ID parameter will be used.","input_types":["Message"],"list":false,"load_from_db":false,"name":"session_id","placeholder":"","required":false,"show":true,"title_case":false,"trace_as_input":true,"trace_as_metadata":true,"type":"str","value":""},"should_store_message":{"_input_type":"BoolInput","advanced":true,"display_name":"Store Messages","dynamic":false,"info":"Store the message in the history.","list":false,"name":"should_store_message","placeholder":"","required":false,"show":true,"title_case":false,"trace_as_metadata":true,"type":"bool","value":true}},"lf_version":"1.0.19"},"type":"ChatInput"},"dragging":false,"height":289,"id":"ChatInput-jV0J9","position":{"x":-605.9700284209953,"y":1197.1469188612098},"positionAbsolute":{"x":-605.9700284209953,"y":1197.1469188612098},"selected":false,"type":"genericNode","width":384},{"data":{"description":"Create a prompt template with dynamic variables. If using an OutputParser, you must include {format_instructions} as an additional variable.","display_name":"Prompt","id":"Prompt-lGhTS","node":{"template":{"_type":"Component","code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langflow.base.prompts.api_utils import process_prompt_template\nfrom langflow.custom import Component\nfrom langflow.inputs.inputs import DefaultPromptField\nfrom langflow.io import Output, PromptInput\nfrom langflow.schema.message import Message\nfrom langflow.template.utils import update_template_values\n\n\nclass PromptComponent(Component):\n    display_name: str = \"Prompt\"\n    description: str = \"Create a prompt template with dynamic variables.\"\n    icon = \"prompts\"\n    trace_type = \"prompt\"\n    name = \"Prompt\"\n\n    inputs = [\n        PromptInput(name=\"template\", display_name=\"Template\"),\n    ]\n\n    outputs = [\n        Output(display_name=\"Prompt Message\", name=\"prompt\", method=\"build_prompt\"),\n    ]\n\n    async def build_prompt(\n        self,\n    ) -> Message:\n        prompt = await Message.from_template_and_variables(**self._attributes)\n        self.status = prompt.text\n        return prompt\n\n    def _update_template(self, frontend_node: dict):\n        prompt_template = frontend_node[\"template\"][\"template\"][\"value\"]\n        custom_fields = frontend_node[\"custom_fields\"]\n        frontend_node_template = frontend_node[\"template\"]\n        _ = process_prompt_template(\n            template=prompt_template,\n            name=\"template\",\n            custom_fields=custom_fields,\n            frontend_node_template=frontend_node_template,\n        )\n        return frontend_node\n\n    def post_code_processing(self, new_frontend_node: dict, current_frontend_node: dict):\n        \"\"\"\n        This function is called after the code validation is done.\n        \"\"\"\n        frontend_node = super().post_code_processing(new_frontend_node, current_frontend_node)\n        template = frontend_node[\"template\"][\"template\"][\"value\"]\n        # Kept it duplicated for backwards compatibility\n        _ = process_prompt_template(\n            template=template,\n            name=\"template\",\n            custom_fields=frontend_node[\"custom_fields\"],\n            frontend_node_template=frontend_node[\"template\"],\n        )\n        # Now that template is updated, we need to grab any values that were set in the current_frontend_node\n        # and update the frontend_node with those values\n        update_template_values(new_template=frontend_node, previous_template=current_frontend_node[\"template\"])\n        return frontend_node\n\n    def _get_fallback_input(self, **kwargs):\n        return DefaultPromptField(**kwargs)\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"template":{"trace_as_input":true,"list":false,"required":false,"placeholder":"","show":true,"name":"template","value":"parse question into key points, then output answer as format in chinese.\nEx:\nQ. 告訴我2025/04/23到期的債券類型產品。 包含產品名稱及到期日。\nA.\n資料條件：\n1. 時間：2025/04/23到期\n2. 債券類型\n3. 產品\n資料欄位：\n1. 產品名稱\n2. 到期日\n\nQ. 告訴我2021年全年銷售總額第二高的基金類型的產品。 包含產品名稱與總銷售金額。\nA.\n資料條件：\n1. 時間：2021年\n2. 全年銷售總額\n3. 第二高\n4. 基金類型\n5. 產品\n資料欄位：\n1. 產品名稱\n2. 總銷售金額\n\nQ. 告訴我Email為tracy35@example.org的客戶最近一次購買的產品，包含產品名稱與購買日期。\nA.\n資料條件：\n1. Email為tracy35@example.org\n2. 客戶\n3. 最近一次\n4. 購買的產品\n資料欄位：\n1. 產品名稱\n2. 購買日期\n\nQ. 告訴我所有出生日期在1990年前的員工的總業績金額。包含員工姓名與業績總額。\nA.\n資料條件：\n1. 所有\n2. 出生日期在1990年前\n3. 員工\n4. 總業績金額\n資料欄位：\n1. 員工姓名\n2. 業績總額\n\nQ. 告訴我所有年齡大於50歲客戶的交易記錄，包含客戶姓名與每筆交易金額。\nA.\n資料條件：\n1. 所有\n2. 年齡大於50歲\n3. 客戶\n4. 交易記錄\n資料欄位：\n1. 客戶姓名\n2. 每筆交易金額\n\nQ. 告訴我員工賴芯菡已完成的交易筆數。包含員工姓名。\nA.\n資料條件：\n1. 員工：賴芯菡\n2. 已完成\n3. 交易筆數\n資料欄位：\n1. 員工姓名\n\nUser: {user_input}","display_name":"Template","advanced":false,"dynamic":false,"info":"","title_case":false,"type":"prompt","_input_type":"PromptInput","load_from_db":false},"user_input":{"field_type":"str","required":false,"placeholder":"","list":false,"show":true,"multiline":true,"value":"","fileTypes":[],"file_path":"","name":"user_input","display_name":"user_input","advanced":false,"input_types":["Message","Text"],"dynamic":false,"info":"","load_from_db":false,"title_case":false,"type":"str"}},"description":"Create a prompt template with dynamic variables.","icon":"prompts","is_input":null,"is_output":null,"is_composition":null,"base_classes":["Message"],"name":"","display_name":"Prompt","documentation":"","custom_fields":{"template":["user_input"]},"output_types":[],"full_path":null,"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"prompt","hidden":null,"display_name":"Prompt Message","method":"build_prompt","value":"__UNDEFINED__","cache":true,"required_inputs":null}],"field_order":["template"],"beta":false,"error":null,"edited":false,"metadata":{},"lf_version":"1.0.19"},"type":"Prompt"},"dragging":false,"height":391,"id":"Prompt-lGhTS","position":{"x":-117.59714035565037,"y":657.011700208914},"positionAbsolute":{"x":-117.59714035565037,"y":657.011700208914},"selected":false,"type":"genericNode","width":384},{"data":{"description":"Display a chat message in the Playground.\n\n💡 Click the ▶️ to run the flow.","display_name":"Chat Output","id":"ChatOutput-zJS5p","node":{"base_classes":["Message"],"beta":false,"conditional_paths":[],"custom_fields":{},"description":"Display a chat message in the Playground.\n\n💡 Click the ▶️ to run the flow.","display_name":"Chat Output","documentation":"","edited":false,"field_order":["input_value","store_message","sender","sender_name","session_id","data_template"],"frozen":false,"icon":"ChatOutput","metadata":{},"output_types":[],"outputs":[{"cache":true,"display_name":"Message","method":"message_response","name":"message","selected":"Message","types":["Message"],"value":"__UNDEFINED__"}],"pinned":false,"template":{"_type":"Component","code":{"advanced":true,"dynamic":true,"fileTypes":[],"file_path":"","info":"","list":false,"load_from_db":false,"multiline":true,"name":"code","password":false,"placeholder":"","required":true,"show":true,"title_case":false,"type":"code","value":"from langflow.base.io.chat import ChatComponent\nfrom langflow.inputs import BoolInput\nfrom langflow.io import DropdownInput, MessageTextInput, Output\nfrom langflow.memory import store_message\nfrom langflow.schema.message import Message\nfrom langflow.utils.constants import MESSAGE_SENDER_AI, MESSAGE_SENDER_NAME_AI, MESSAGE_SENDER_USER\n\n\nclass ChatOutput(ChatComponent):\n    display_name = \"Chat Output\"\n    description = \"Display a chat message in the Playground.\"\n    icon = \"ChatOutput\"\n    name = \"ChatOutput\"\n\n    inputs = [\n        MessageTextInput(\n            name=\"input_value\",\n            display_name=\"Text\",\n            info=\"Message to be passed as output.\",\n        ),\n        BoolInput(\n            name=\"should_store_message\",\n            display_name=\"Store Messages\",\n            info=\"Store the message in the history.\",\n            value=True,\n            advanced=True,\n        ),\n        DropdownInput(\n            name=\"sender\",\n            display_name=\"Sender Type\",\n            options=[MESSAGE_SENDER_AI, MESSAGE_SENDER_USER],\n            value=MESSAGE_SENDER_AI,\n            advanced=True,\n            info=\"Type of sender.\",\n        ),\n        MessageTextInput(\n            name=\"sender_name\",\n            display_name=\"Sender Name\",\n            info=\"Name of the sender.\",\n            value=MESSAGE_SENDER_NAME_AI,\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"session_id\",\n            display_name=\"Session ID\",\n            info=\"The session ID of the chat. If empty, the current session ID parameter will be used.\",\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"data_template\",\n            display_name=\"Data Template\",\n            value=\"{text}\",\n            advanced=True,\n            info=\"Template to convert Data to Text. If left empty, it will be dynamically set to the Data's text key.\",\n        ),\n    ]\n    outputs = [\n        Output(display_name=\"Message\", name=\"message\", method=\"message_response\"),\n    ]\n\n    def message_response(self) -> Message:\n        message = Message(\n            text=self.input_value,\n            sender=self.sender,\n            sender_name=self.sender_name,\n            session_id=self.session_id,\n        )\n        if (\n            self.session_id\n            and isinstance(message, Message)\n            and isinstance(message.text, str)\n            and self.should_store_message\n        ):\n            store_message(\n                message,\n                flow_id=self.graph.flow_id,\n            )\n            self.message.value = message\n\n        self.status = message\n        return message\n"},"data_template":{"advanced":true,"display_name":"Data Template","dynamic":false,"info":"Template to convert Data to Text. If left empty, it will be dynamically set to the Data's text key.","input_types":["Message"],"list":false,"load_from_db":false,"name":"data_template","placeholder":"","required":false,"show":true,"title_case":false,"trace_as_input":true,"trace_as_metadata":true,"type":"str","value":"{text}"},"input_value":{"advanced":false,"display_name":"Text","dynamic":false,"info":"Message to be passed as output.","input_types":["Message"],"list":false,"load_from_db":false,"name":"input_value","placeholder":"","required":false,"show":true,"title_case":false,"trace_as_input":true,"trace_as_metadata":true,"type":"str","value":""},"sender":{"advanced":true,"display_name":"Sender Type","dynamic":false,"info":"Type of sender.","name":"sender","options":["Machine","User"],"placeholder":"","required":false,"show":true,"title_case":false,"trace_as_metadata":true,"type":"str","value":"Machine"},"sender_name":{"advanced":true,"display_name":"Sender Name","dynamic":false,"info":"Name of the sender.","input_types":["Message"],"list":false,"load_from_db":false,"name":"sender_name","placeholder":"","required":false,"show":true,"title_case":false,"trace_as_input":true,"trace_as_metadata":true,"type":"str","value":"AI"},"session_id":{"advanced":true,"display_name":"Session ID","dynamic":false,"info":"The session ID of the chat. If empty, the current session ID parameter will be used.","input_types":["Message"],"list":false,"load_from_db":false,"name":"session_id","placeholder":"","required":false,"show":true,"title_case":false,"trace_as_input":true,"trace_as_metadata":true,"type":"str","value":""},"should_store_message":{"_input_type":"BoolInput","advanced":true,"display_name":"Store Messages","dynamic":false,"info":"Store the message in the history.","list":false,"name":"should_store_message","placeholder":"","required":false,"show":true,"title_case":false,"trace_as_metadata":true,"type":"bool","value":true}},"lf_version":"1.0.19"},"type":"ChatOutput"},"dragging":false,"height":329,"id":"ChatOutput-zJS5p","position":{"x":4527.178690959507,"y":1134.6395782365792},"positionAbsolute":{"x":4527.178690959507,"y":1134.6395782365792},"selected":false,"type":"genericNode","width":384},{"id":"OllamaModel-kGRvk","type":"genericNode","position":{"x":373.7297779566593,"y":430.1127655501515},"data":{"type":"OllamaModel","node":{"template":{"_type":"Component","output_parser":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"output_parser","value":"","display_name":"Output Parser","advanced":true,"input_types":["OutputParser"],"dynamic":false,"info":"The parser to use to parse the output of the model","title_case":false,"type":"other","_input_type":"HandleInput"},"base_url":{"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"base_url","value":"http://localhost:11434","display_name":"Base URL","advanced":false,"dynamic":false,"info":"Endpoint of the Ollama API. Defaults to 'http://localhost:11434' if not specified.","title_case":false,"type":"str","_input_type":"StrInput"},"code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from typing import Any\nfrom urllib.parse import urljoin\n\nimport httpx\nfrom langchain_community.chat_models import ChatOllama\n\nfrom langflow.base.models.model import LCModelComponent\nfrom langflow.field_typing import LanguageModel\nfrom langflow.inputs.inputs import HandleInput\nfrom langflow.io import BoolInput, DictInput, DropdownInput, FloatInput, IntInput, StrInput\n\n\nclass ChatOllamaComponent(LCModelComponent):\n    display_name = \"Ollama\"\n    description = \"Generate text using Ollama Local LLMs.\"\n    icon = \"Ollama\"\n    name = \"OllamaModel\"\n\n    def update_build_config(self, build_config: dict, field_value: Any, field_name: str | None = None):\n        if field_name == \"mirostat\":\n            if field_value == \"Disabled\":\n                build_config[\"mirostat_eta\"][\"advanced\"] = True\n                build_config[\"mirostat_tau\"][\"advanced\"] = True\n                build_config[\"mirostat_eta\"][\"value\"] = None\n                build_config[\"mirostat_tau\"][\"value\"] = None\n\n            else:\n                build_config[\"mirostat_eta\"][\"advanced\"] = False\n                build_config[\"mirostat_tau\"][\"advanced\"] = False\n\n                if field_value == \"Mirostat 2.0\":\n                    build_config[\"mirostat_eta\"][\"value\"] = 0.2\n                    build_config[\"mirostat_tau\"][\"value\"] = 10\n                else:\n                    build_config[\"mirostat_eta\"][\"value\"] = 0.1\n                    build_config[\"mirostat_tau\"][\"value\"] = 5\n\n        if field_name == \"model_name\":\n            base_url_dict = build_config.get(\"base_url\", {})\n            base_url_load_from_db = base_url_dict.get(\"load_from_db\", False)\n            base_url_value = base_url_dict.get(\"value\")\n            if base_url_load_from_db:\n                base_url_value = self.variables(base_url_value, field_name)\n            elif not base_url_value:\n                base_url_value = \"http://localhost:11434\"\n            build_config[\"model_name\"][\"options\"] = self.get_model(base_url_value)\n        if field_name == \"keep_alive_flag\":\n            if field_value == \"Keep\":\n                build_config[\"keep_alive\"][\"value\"] = \"-1\"\n                build_config[\"keep_alive\"][\"advanced\"] = True\n            elif field_value == \"Immediately\":\n                build_config[\"keep_alive\"][\"value\"] = \"0\"\n                build_config[\"keep_alive\"][\"advanced\"] = True\n            else:\n                build_config[\"keep_alive\"][\"advanced\"] = False\n\n        return build_config\n\n    def get_model(self, base_url_value: str) -> list[str]:\n        try:\n            url = urljoin(base_url_value, \"/api/tags\")\n            with httpx.Client() as client:\n                response = client.get(url)\n                response.raise_for_status()\n                data = response.json()\n\n                return [model[\"name\"] for model in data.get(\"models\", [])]\n        except Exception as e:\n            msg = \"Could not retrieve models. Please, make sure Ollama is running.\"\n            raise ValueError(msg) from e\n\n    inputs = [\n        *LCModelComponent._base_inputs,\n        StrInput(\n            name=\"base_url\",\n            display_name=\"Base URL\",\n            info=\"Endpoint of the Ollama API. Defaults to 'http://localhost:11434' if not specified.\",\n            value=\"http://localhost:11434\",\n        ),\n        DropdownInput(\n            name=\"model_name\",\n            display_name=\"Model Name\",\n            value=\"llama3.1\",\n            info=\"Refer to https://ollama.com/library for more models.\",\n            refresh_button=True,\n        ),\n        FloatInput(\n            name=\"temperature\",\n            display_name=\"Temperature\",\n            value=0.2,\n            info=\"Controls the creativity of model responses.\",\n        ),\n        StrInput(\n            name=\"format\", display_name=\"Format\", info=\"Specify the format of the output (e.g., json).\", advanced=True\n        ),\n        DictInput(name=\"metadata\", display_name=\"Metadata\", info=\"Metadata to add to the run trace.\", advanced=True),\n        DropdownInput(\n            name=\"mirostat\",\n            display_name=\"Mirostat\",\n            options=[\"Disabled\", \"Mirostat\", \"Mirostat 2.0\"],\n            info=\"Enable/disable Mirostat sampling for controlling perplexity.\",\n            value=\"Disabled\",\n            advanced=True,\n            real_time_refresh=True,\n        ),\n        FloatInput(\n            name=\"mirostat_eta\",\n            display_name=\"Mirostat Eta\",\n            info=\"Learning rate for Mirostat algorithm. (Default: 0.1)\",\n            advanced=True,\n        ),\n        FloatInput(\n            name=\"mirostat_tau\",\n            display_name=\"Mirostat Tau\",\n            info=\"Controls the balance between coherence and diversity of the output. (Default: 5.0)\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"num_ctx\",\n            display_name=\"Context Window Size\",\n            info=\"Size of the context window for generating tokens. (Default: 2048)\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"num_gpu\",\n            display_name=\"Number of GPUs\",\n            info=\"Number of GPUs to use for computation. (Default: 1 on macOS, 0 to disable)\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"num_thread\",\n            display_name=\"Number of Threads\",\n            info=\"Number of threads to use during computation. (Default: detected for optimal performance)\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"repeat_last_n\",\n            display_name=\"Repeat Last N\",\n            info=\"How far back the model looks to prevent repetition. (Default: 64, 0 = disabled, -1 = num_ctx)\",\n            advanced=True,\n        ),\n        FloatInput(\n            name=\"repeat_penalty\",\n            display_name=\"Repeat Penalty\",\n            info=\"Penalty for repetitions in generated text. (Default: 1.1)\",\n            advanced=True,\n        ),\n        FloatInput(name=\"tfs_z\", display_name=\"TFS Z\", info=\"Tail free sampling value. (Default: 1)\", advanced=True),\n        IntInput(name=\"timeout\", display_name=\"Timeout\", info=\"Timeout for the request stream.\", advanced=True),\n        IntInput(\n            name=\"top_k\", display_name=\"Top K\", info=\"Limits token selection to top K. (Default: 40)\", advanced=True\n        ),\n        FloatInput(name=\"top_p\", display_name=\"Top P\", info=\"Works together with top-k. (Default: 0.9)\", advanced=True),\n        BoolInput(name=\"verbose\", display_name=\"Verbose\", info=\"Whether to print out response text.\"),\n        StrInput(\n            name=\"tags\",\n            display_name=\"Tags\",\n            info=\"Comma-separated list of tags to add to the run trace.\",\n            advanced=True,\n        ),\n        StrInput(\n            name=\"stop_tokens\",\n            display_name=\"Stop Tokens\",\n            info=\"Comma-separated list of tokens to signal the model to stop generating text.\",\n            advanced=True,\n        ),\n        StrInput(name=\"system\", display_name=\"System\", info=\"System to use for generating text.\", advanced=True),\n        StrInput(name=\"template\", display_name=\"Template\", info=\"Template to use for generating text.\", advanced=True),\n        HandleInput(\n            name=\"output_parser\",\n            display_name=\"Output Parser\",\n            info=\"The parser to use to parse the output of the model\",\n            advanced=True,\n            input_types=[\"OutputParser\"],\n        ),\n    ]\n\n    def build_model(self) -> LanguageModel:  # type: ignore[type-var]\n        # Mapping mirostat settings to their corresponding values\n        mirostat_options = {\"Mirostat\": 1, \"Mirostat 2.0\": 2}\n\n        # Default to 0 for 'Disabled'\n        mirostat_value = mirostat_options.get(self.mirostat, 0)\n\n        # Set mirostat_eta and mirostat_tau to None if mirostat is disabled\n        if mirostat_value == 0:\n            mirostat_eta = None\n            mirostat_tau = None\n        else:\n            mirostat_eta = self.mirostat_eta\n            mirostat_tau = self.mirostat_tau\n\n        # Mapping system settings to their corresponding values\n        llm_params = {\n            \"base_url\": self.base_url,\n            \"model\": self.model_name,\n            \"mirostat\": mirostat_value,\n            \"format\": self.format,\n            \"metadata\": self.metadata,\n            \"tags\": self.tags.split(\",\") if self.tags else None,\n            \"mirostat_eta\": mirostat_eta,\n            \"mirostat_tau\": mirostat_tau,\n            \"num_ctx\": self.num_ctx or None,\n            \"num_gpu\": self.num_gpu or None,\n            \"num_thread\": self.num_thread or None,\n            \"repeat_last_n\": self.repeat_last_n or None,\n            \"repeat_penalty\": self.repeat_penalty or None,\n            \"temperature\": self.temperature or None,\n            \"stop\": self.stop_tokens.split(\",\") if self.stop_tokens else None,\n            \"system\": self.system,\n            \"template\": self.template,\n            \"tfs_z\": self.tfs_z or None,\n            \"timeout\": self.timeout or None,\n            \"top_k\": self.top_k or None,\n            \"top_p\": self.top_p or None,\n            \"verbose\": self.verbose,\n        }\n\n        # Remove parameters with None values\n        llm_params = {k: v for k, v in llm_params.items() if v is not None}\n\n        try:\n            output = ChatOllama(**llm_params)\n        except Exception as e:\n            msg = \"Could not initialize Ollama LLM.\"\n            raise ValueError(msg) from e\n\n        return output\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"format":{"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"format","value":"","display_name":"Format","advanced":true,"dynamic":false,"info":"Specify the format of the output (e.g., json).","title_case":false,"type":"str","_input_type":"StrInput"},"input_value":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"input_value","value":"","display_name":"Input","advanced":false,"input_types":["Message"],"dynamic":false,"info":"","title_case":false,"type":"str","_input_type":"MessageInput"},"metadata":{"trace_as_input":true,"list":false,"required":false,"placeholder":"","show":true,"name":"metadata","value":{},"display_name":"Metadata","advanced":true,"dynamic":false,"info":"Metadata to add to the run trace.","title_case":false,"type":"dict","_input_type":"DictInput"},"mirostat":{"trace_as_metadata":true,"options":["Disabled","Mirostat","Mirostat 2.0"],"combobox":false,"required":false,"placeholder":"","show":true,"name":"mirostat","value":"Disabled","display_name":"Mirostat","advanced":true,"dynamic":false,"info":"Enable/disable Mirostat sampling for controlling perplexity.","real_time_refresh":true,"title_case":false,"type":"str","_input_type":"DropdownInput"},"mirostat_eta":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"mirostat_eta","value":"","display_name":"Mirostat Eta","advanced":true,"dynamic":false,"info":"Learning rate for Mirostat algorithm. (Default: 0.1)","title_case":false,"type":"float","_input_type":"FloatInput"},"mirostat_tau":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"mirostat_tau","value":"","display_name":"Mirostat Tau","advanced":true,"dynamic":false,"info":"Controls the balance between coherence and diversity of the output. (Default: 5.0)","title_case":false,"type":"float","_input_type":"FloatInput"},"model_name":{"trace_as_metadata":true,"options":["aerok/zpoint_large_embedding_zh:latest","gemma2:latest"],"combobox":false,"required":false,"placeholder":"","show":true,"name":"model_name","value":"gemma2:latest","display_name":"Model Name","advanced":false,"dynamic":false,"info":"Refer to https://ollama.com/library for more models.","refresh_button":true,"title_case":false,"type":"str","_input_type":"DropdownInput"},"num_ctx":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"num_ctx","value":"","display_name":"Context Window Size","advanced":true,"dynamic":false,"info":"Size of the context window for generating tokens. (Default: 2048)","title_case":false,"type":"int","_input_type":"IntInput"},"num_gpu":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"num_gpu","value":"","display_name":"Number of GPUs","advanced":true,"dynamic":false,"info":"Number of GPUs to use for computation. (Default: 1 on macOS, 0 to disable)","title_case":false,"type":"int","_input_type":"IntInput"},"num_thread":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"num_thread","value":"","display_name":"Number of Threads","advanced":true,"dynamic":false,"info":"Number of threads to use during computation. (Default: detected for optimal performance)","title_case":false,"type":"int","_input_type":"IntInput"},"repeat_last_n":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"repeat_last_n","value":"","display_name":"Repeat Last N","advanced":true,"dynamic":false,"info":"How far back the model looks to prevent repetition. (Default: 64, 0 = disabled, -1 = num_ctx)","title_case":false,"type":"int","_input_type":"IntInput"},"repeat_penalty":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"repeat_penalty","value":"","display_name":"Repeat Penalty","advanced":true,"dynamic":false,"info":"Penalty for repetitions in generated text. (Default: 1.1)","title_case":false,"type":"float","_input_type":"FloatInput"},"stop_tokens":{"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"stop_tokens","value":"","display_name":"Stop Tokens","advanced":true,"dynamic":false,"info":"Comma-separated list of tokens to signal the model to stop generating text.","title_case":false,"type":"str","_input_type":"StrInput"},"stream":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"stream","value":false,"display_name":"Stream","advanced":true,"dynamic":false,"info":"Stream the response from the model. Streaming works only in Chat.","title_case":false,"type":"bool","_input_type":"BoolInput"},"system":{"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"system","value":"","display_name":"System","advanced":true,"dynamic":false,"info":"System to use for generating text.","title_case":false,"type":"str","_input_type":"StrInput"},"system_message":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"system_message","value":"","display_name":"System Message","advanced":true,"input_types":["Message"],"dynamic":false,"info":"System message to pass to the model.","title_case":false,"type":"str","_input_type":"MessageTextInput"},"tags":{"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"tags","value":"","display_name":"Tags","advanced":true,"dynamic":false,"info":"Comma-separated list of tags to add to the run trace.","title_case":false,"type":"str","_input_type":"StrInput"},"temperature":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"temperature","value":0,"display_name":"Temperature","advanced":false,"dynamic":false,"info":"Controls the creativity of model responses.","title_case":false,"type":"float","_input_type":"FloatInput"},"template":{"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"template","value":"","display_name":"Template","advanced":true,"dynamic":false,"info":"Template to use for generating text.","title_case":false,"type":"str","_input_type":"StrInput"},"tfs_z":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"tfs_z","value":"","display_name":"TFS Z","advanced":true,"dynamic":false,"info":"Tail free sampling value. (Default: 1)","title_case":false,"type":"float","_input_type":"FloatInput"},"timeout":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"timeout","value":"","display_name":"Timeout","advanced":true,"dynamic":false,"info":"Timeout for the request stream.","title_case":false,"type":"int","_input_type":"IntInput"},"top_k":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"top_k","value":"","display_name":"Top K","advanced":true,"dynamic":false,"info":"Limits token selection to top K. (Default: 40)","title_case":false,"type":"int","_input_type":"IntInput"},"top_p":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"top_p","value":"","display_name":"Top P","advanced":true,"dynamic":false,"info":"Works together with top-k. (Default: 0.9)","title_case":false,"type":"float","_input_type":"FloatInput"},"verbose":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"verbose","value":true,"display_name":"Verbose","advanced":false,"dynamic":false,"info":"Whether to print out response text.","title_case":false,"type":"bool","_input_type":"BoolInput"}},"description":"Generate text using Ollama Local LLMs.","icon":"Ollama","base_classes":["LanguageModel","Message"],"display_name":"Ollama","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"text_output","display_name":"Text","method":"text_response","value":"__UNDEFINED__","cache":true,"required_inputs":["input_value","stream","system_message"]},{"types":["LanguageModel"],"selected":"LanguageModel","name":"model_output","display_name":"Language Model","method":"build_model","value":"__UNDEFINED__","cache":true,"required_inputs":["base_url","format","metadata","mirostat","mirostat_eta","mirostat_tau","model_name","num_ctx","num_gpu","num_thread","repeat_last_n","repeat_penalty","stop_tokens","system","tags","temperature","template","tfs_z","timeout","top_k","top_p","verbose"]}],"field_order":["input_value","system_message","stream","base_url","model_name","temperature","format","metadata","mirostat","mirostat_eta","mirostat_tau","num_ctx","num_gpu","num_thread","repeat_last_n","repeat_penalty","tfs_z","timeout","top_k","top_p","verbose","tags","stop_tokens","system","template","output_parser"],"beta":false,"edited":false,"metadata":{},"lf_version":"1.0.19"},"id":"OllamaModel-kGRvk"},"selected":false,"width":384,"height":667,"positionAbsolute":{"x":373.7297779566593,"y":430.1127655501515},"dragging":false},{"id":"OllamaEmbeddings-uJKV7","type":"genericNode","position":{"x":910.3685001988754,"y":1053.972832307663},"data":{"type":"OllamaEmbeddings","node":{"template":{"_type":"Component","base_url":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"base_url","value":"http://localhost:11434","display_name":"Ollama Base URL","advanced":false,"input_types":["Message"],"dynamic":false,"info":"","title_case":false,"type":"str","_input_type":"MessageTextInput"},"code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langchain_community.embeddings import OllamaEmbeddings\n\nfrom langflow.base.models.model import LCModelComponent\nfrom langflow.field_typing import Embeddings\nfrom langflow.io import FloatInput, MessageTextInput, Output\n\n\nclass OllamaEmbeddingsComponent(LCModelComponent):\n    display_name: str = \"Ollama Embeddings\"\n    description: str = \"Generate embeddings using Ollama models.\"\n    documentation = \"https://python.langchain.com/docs/integrations/text_embedding/ollama\"\n    icon = \"Ollama\"\n    name = \"OllamaEmbeddings\"\n\n    inputs = [\n        MessageTextInput(\n            name=\"model\",\n            display_name=\"Ollama Model\",\n            value=\"llama3.1\",\n        ),\n        MessageTextInput(\n            name=\"base_url\",\n            display_name=\"Ollama Base URL\",\n            value=\"http://localhost:11434\",\n        ),\n        FloatInput(\n            name=\"temperature\",\n            display_name=\"Model Temperature\",\n            value=0.1,\n            advanced=True,\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Embeddings\", name=\"embeddings\", method=\"build_embeddings\"),\n    ]\n\n    def build_embeddings(self) -> Embeddings:\n        try:\n            output = OllamaEmbeddings(\n                model=self.model,\n                base_url=self.base_url,\n                temperature=self.temperature,\n            )\n        except Exception as e:\n            msg = \"Could not connect to Ollama API.\"\n            raise ValueError(msg) from e\n        return output\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"model":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"model","value":"aerok/zpoint_large_embedding_zh:latest","display_name":"Ollama Model","advanced":false,"input_types":["Message"],"dynamic":false,"info":"","title_case":false,"type":"str","_input_type":"MessageTextInput"},"temperature":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"temperature","value":0.1,"display_name":"Model Temperature","advanced":true,"dynamic":false,"info":"","title_case":false,"type":"float","_input_type":"FloatInput"}},"description":"Generate embeddings using Ollama models.","icon":"Ollama","base_classes":["Embeddings"],"display_name":"Ollama Embeddings","documentation":"https://python.langchain.com/docs/integrations/text_embedding/ollama","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Embeddings"],"selected":"Embeddings","name":"embeddings","display_name":"Embeddings","method":"build_embeddings","value":"__UNDEFINED__","cache":true}],"field_order":["model","base_url","temperature"],"beta":false,"edited":false,"metadata":{},"lf_version":"1.0.19"},"id":"OllamaEmbeddings-uJKV7"},"selected":false,"width":384,"height":383,"positionAbsolute":{"x":910.3685001988754,"y":1053.972832307663},"dragging":false},{"id":"CustomComponent-RFa62","type":"genericNode","position":{"x":1422.4304224206903,"y":757.9841894998194},"data":{"type":"CustomComponent","node":{"template":{"_type":"Component","embedding":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"embedding","value":"","display_name":"Embedding","advanced":false,"input_types":["Embeddings"],"dynamic":false,"info":"","title_case":false,"type":"other","_input_type":"HandleInput"},"code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"# from langflow.field_typing import Data\nfrom langchain_community.vectorstores import FAISS\n\nfrom langflow.custom import Component\nfrom langflow.io import StrInput, MultilineInput, IntInput, HandleInput, Output\nfrom langflow.schema import Data\n\nimport os\nimport re\n\nclass CustomComponent(Component):\n    display_name = \"Custom Component\"\n    description = \"Use as a template to create your own component.\"\n    documentation: str = \"http://docs.langflow.org/components/custom\"\n    icon = \"custom_components\"\n    name = \"CustomComponent\"\n\n    inputs = [\n        MultilineInput(\n            name=\"metadata_directory\",\n            display_name=\"Metadata Directory\",\n            info=\"Path of Metadata\",\n        ),\n        MultilineInput(\n            name=\"keywords\",\n            display_name=\"Keywords\",\n        ),\n        HandleInput(name=\"embedding\", display_name=\"Embedding\", input_types=[\"Embeddings\"]),\n        IntInput(\n            name=\"topK\",\n            display_name=\"TopK\",\n        )\n    ]\n\n    outputs = [\n        Output(display_name=\"Output\", name=\"output\", method=\"FAISS_retriever\"),\n    ]\n\n    def FAISS_retriever(self) -> Message:\n        \n        pattern = r'\\d+\\.\\s*(.*?)\\n'\n        keywords_list = re.findall(pattern, self.keywords)\n        keywords_list = [keyword.strip() for keyword in keywords_list] \n        \n        table_list = []\n        file_name = \"/database_table_meta.txt\"\n        \n        for database in os.listdir(self.metadata_directory):\n            try:\n                with open(self.metadata_directory + database + file_name, 'r', encoding='utf-8') as file:\n                    lines = file.read().split('\\n')\n                    table_list += lines[1:]\n            except:\n                pass\n        \n        table_list = [item for item in table_list if item != '']\n        table_vectors = FAISS.from_texts(table_list, self.embedding)\n        \n        table_inform_list = []\n        for keyword in keywords_list:\n            for result in table_vectors.similarity_search_with_score(keyword, k = self.topK):\n                table_inform_list.append(result[0].page_content)\n                \n        tables_set = set(table_inform_list)\n        database_inform = 'database | table | metadata\\n' + '\\n'.join(sorted(list(tables_set)))\n        \n        return Message(\n            text=database_inform,\n        )\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"keywords":{"trace_as_input":true,"multiline":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"keywords","value":"","display_name":"Keywords","advanced":false,"input_types":["Message"],"dynamic":false,"info":"","title_case":false,"type":"str","_input_type":"MultilineInput"},"metadata_directory":{"trace_as_input":true,"multiline":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"metadata_directory","value":"","display_name":"Metadata Directory","advanced":false,"input_types":["Message"],"dynamic":false,"info":"Path of Metadata","title_case":false,"type":"str","_input_type":"MultilineInput"},"topK":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"topK","value":5,"display_name":"TopK","advanced":false,"dynamic":false,"info":"","title_case":false,"type":"int","_input_type":"IntInput","load_from_db":false}},"description":"Use as a template to create your own component.","icon":"custom_components","base_classes":["Message"],"display_name":"Custom Component","documentation":"http://docs.langflow.org/components/custom","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"output","display_name":"Output","method":"FAISS_retriever","value":"__UNDEFINED__","cache":true}],"field_order":["metadata_directory","keywords","embedding","topK"],"beta":false,"edited":true,"metadata":{},"lf_version":"1.0.19"},"id":"CustomComponent-RFa62"},"selected":false,"width":384,"height":525,"dragging":false,"positionAbsolute":{"x":1422.4304224206903,"y":757.9841894998194}},{"id":"OllamaModel-cVR6p","type":"genericNode","position":{"x":2462.4615710061175,"y":1013.0091288873379},"data":{"type":"OllamaModel","node":{"template":{"_type":"Component","output_parser":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"output_parser","value":"","display_name":"Output Parser","advanced":true,"input_types":["OutputParser"],"dynamic":false,"info":"The parser to use to parse the output of the model","title_case":false,"type":"other","_input_type":"HandleInput"},"base_url":{"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"base_url","value":"http://localhost:11434","display_name":"Base URL","advanced":false,"dynamic":false,"info":"Endpoint of the Ollama API. Defaults to 'http://localhost:11434' if not specified.","title_case":false,"type":"str","_input_type":"StrInput"},"code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from typing import Any\nfrom urllib.parse import urljoin\n\nimport httpx\nfrom langchain_community.chat_models import ChatOllama\n\nfrom langflow.base.models.model import LCModelComponent\nfrom langflow.field_typing import LanguageModel\nfrom langflow.inputs.inputs import HandleInput\nfrom langflow.io import BoolInput, DictInput, DropdownInput, FloatInput, IntInput, StrInput\n\n\nclass ChatOllamaComponent(LCModelComponent):\n    display_name = \"Ollama\"\n    description = \"Generate text using Ollama Local LLMs.\"\n    icon = \"Ollama\"\n    name = \"OllamaModel\"\n\n    def update_build_config(self, build_config: dict, field_value: Any, field_name: str | None = None):\n        if field_name == \"mirostat\":\n            if field_value == \"Disabled\":\n                build_config[\"mirostat_eta\"][\"advanced\"] = True\n                build_config[\"mirostat_tau\"][\"advanced\"] = True\n                build_config[\"mirostat_eta\"][\"value\"] = None\n                build_config[\"mirostat_tau\"][\"value\"] = None\n\n            else:\n                build_config[\"mirostat_eta\"][\"advanced\"] = False\n                build_config[\"mirostat_tau\"][\"advanced\"] = False\n\n                if field_value == \"Mirostat 2.0\":\n                    build_config[\"mirostat_eta\"][\"value\"] = 0.2\n                    build_config[\"mirostat_tau\"][\"value\"] = 10\n                else:\n                    build_config[\"mirostat_eta\"][\"value\"] = 0.1\n                    build_config[\"mirostat_tau\"][\"value\"] = 5\n\n        if field_name == \"model_name\":\n            base_url_dict = build_config.get(\"base_url\", {})\n            base_url_load_from_db = base_url_dict.get(\"load_from_db\", False)\n            base_url_value = base_url_dict.get(\"value\")\n            if base_url_load_from_db:\n                base_url_value = self.variables(base_url_value, field_name)\n            elif not base_url_value:\n                base_url_value = \"http://localhost:11434\"\n            build_config[\"model_name\"][\"options\"] = self.get_model(base_url_value)\n        if field_name == \"keep_alive_flag\":\n            if field_value == \"Keep\":\n                build_config[\"keep_alive\"][\"value\"] = \"-1\"\n                build_config[\"keep_alive\"][\"advanced\"] = True\n            elif field_value == \"Immediately\":\n                build_config[\"keep_alive\"][\"value\"] = \"0\"\n                build_config[\"keep_alive\"][\"advanced\"] = True\n            else:\n                build_config[\"keep_alive\"][\"advanced\"] = False\n\n        return build_config\n\n    def get_model(self, base_url_value: str) -> list[str]:\n        try:\n            url = urljoin(base_url_value, \"/api/tags\")\n            with httpx.Client() as client:\n                response = client.get(url)\n                response.raise_for_status()\n                data = response.json()\n\n                return [model[\"name\"] for model in data.get(\"models\", [])]\n        except Exception as e:\n            msg = \"Could not retrieve models. Please, make sure Ollama is running.\"\n            raise ValueError(msg) from e\n\n    inputs = [\n        *LCModelComponent._base_inputs,\n        StrInput(\n            name=\"base_url\",\n            display_name=\"Base URL\",\n            info=\"Endpoint of the Ollama API. Defaults to 'http://localhost:11434' if not specified.\",\n            value=\"http://localhost:11434\",\n        ),\n        DropdownInput(\n            name=\"model_name\",\n            display_name=\"Model Name\",\n            value=\"llama3.1\",\n            info=\"Refer to https://ollama.com/library for more models.\",\n            refresh_button=True,\n        ),\n        FloatInput(\n            name=\"temperature\",\n            display_name=\"Temperature\",\n            value=0.2,\n            info=\"Controls the creativity of model responses.\",\n        ),\n        StrInput(\n            name=\"format\", display_name=\"Format\", info=\"Specify the format of the output (e.g., json).\", advanced=True\n        ),\n        DictInput(name=\"metadata\", display_name=\"Metadata\", info=\"Metadata to add to the run trace.\", advanced=True),\n        DropdownInput(\n            name=\"mirostat\",\n            display_name=\"Mirostat\",\n            options=[\"Disabled\", \"Mirostat\", \"Mirostat 2.0\"],\n            info=\"Enable/disable Mirostat sampling for controlling perplexity.\",\n            value=\"Disabled\",\n            advanced=True,\n            real_time_refresh=True,\n        ),\n        FloatInput(\n            name=\"mirostat_eta\",\n            display_name=\"Mirostat Eta\",\n            info=\"Learning rate for Mirostat algorithm. (Default: 0.1)\",\n            advanced=True,\n        ),\n        FloatInput(\n            name=\"mirostat_tau\",\n            display_name=\"Mirostat Tau\",\n            info=\"Controls the balance between coherence and diversity of the output. (Default: 5.0)\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"num_ctx\",\n            display_name=\"Context Window Size\",\n            info=\"Size of the context window for generating tokens. (Default: 2048)\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"num_gpu\",\n            display_name=\"Number of GPUs\",\n            info=\"Number of GPUs to use for computation. (Default: 1 on macOS, 0 to disable)\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"num_thread\",\n            display_name=\"Number of Threads\",\n            info=\"Number of threads to use during computation. (Default: detected for optimal performance)\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"repeat_last_n\",\n            display_name=\"Repeat Last N\",\n            info=\"How far back the model looks to prevent repetition. (Default: 64, 0 = disabled, -1 = num_ctx)\",\n            advanced=True,\n        ),\n        FloatInput(\n            name=\"repeat_penalty\",\n            display_name=\"Repeat Penalty\",\n            info=\"Penalty for repetitions in generated text. (Default: 1.1)\",\n            advanced=True,\n        ),\n        FloatInput(name=\"tfs_z\", display_name=\"TFS Z\", info=\"Tail free sampling value. (Default: 1)\", advanced=True),\n        IntInput(name=\"timeout\", display_name=\"Timeout\", info=\"Timeout for the request stream.\", advanced=True),\n        IntInput(\n            name=\"top_k\", display_name=\"Top K\", info=\"Limits token selection to top K. (Default: 40)\", advanced=True\n        ),\n        FloatInput(name=\"top_p\", display_name=\"Top P\", info=\"Works together with top-k. (Default: 0.9)\", advanced=True),\n        BoolInput(name=\"verbose\", display_name=\"Verbose\", info=\"Whether to print out response text.\"),\n        StrInput(\n            name=\"tags\",\n            display_name=\"Tags\",\n            info=\"Comma-separated list of tags to add to the run trace.\",\n            advanced=True,\n        ),\n        StrInput(\n            name=\"stop_tokens\",\n            display_name=\"Stop Tokens\",\n            info=\"Comma-separated list of tokens to signal the model to stop generating text.\",\n            advanced=True,\n        ),\n        StrInput(name=\"system\", display_name=\"System\", info=\"System to use for generating text.\", advanced=True),\n        StrInput(name=\"template\", display_name=\"Template\", info=\"Template to use for generating text.\", advanced=True),\n        HandleInput(\n            name=\"output_parser\",\n            display_name=\"Output Parser\",\n            info=\"The parser to use to parse the output of the model\",\n            advanced=True,\n            input_types=[\"OutputParser\"],\n        ),\n    ]\n\n    def build_model(self) -> LanguageModel:  # type: ignore[type-var]\n        # Mapping mirostat settings to their corresponding values\n        mirostat_options = {\"Mirostat\": 1, \"Mirostat 2.0\": 2}\n\n        # Default to 0 for 'Disabled'\n        mirostat_value = mirostat_options.get(self.mirostat, 0)\n\n        # Set mirostat_eta and mirostat_tau to None if mirostat is disabled\n        if mirostat_value == 0:\n            mirostat_eta = None\n            mirostat_tau = None\n        else:\n            mirostat_eta = self.mirostat_eta\n            mirostat_tau = self.mirostat_tau\n\n        # Mapping system settings to their corresponding values\n        llm_params = {\n            \"base_url\": self.base_url,\n            \"model\": self.model_name,\n            \"mirostat\": mirostat_value,\n            \"format\": self.format,\n            \"metadata\": self.metadata,\n            \"tags\": self.tags.split(\",\") if self.tags else None,\n            \"mirostat_eta\": mirostat_eta,\n            \"mirostat_tau\": mirostat_tau,\n            \"num_ctx\": self.num_ctx or None,\n            \"num_gpu\": self.num_gpu or None,\n            \"num_thread\": self.num_thread or None,\n            \"repeat_last_n\": self.repeat_last_n or None,\n            \"repeat_penalty\": self.repeat_penalty or None,\n            \"temperature\": self.temperature or None,\n            \"stop\": self.stop_tokens.split(\",\") if self.stop_tokens else None,\n            \"system\": self.system,\n            \"template\": self.template,\n            \"tfs_z\": self.tfs_z or None,\n            \"timeout\": self.timeout or None,\n            \"top_k\": self.top_k or None,\n            \"top_p\": self.top_p or None,\n            \"verbose\": self.verbose,\n        }\n\n        # Remove parameters with None values\n        llm_params = {k: v for k, v in llm_params.items() if v is not None}\n\n        try:\n            output = ChatOllama(**llm_params)\n        except Exception as e:\n            msg = \"Could not initialize Ollama LLM.\"\n            raise ValueError(msg) from e\n\n        return output\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"format":{"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"format","value":"","display_name":"Format","advanced":true,"dynamic":false,"info":"Specify the format of the output (e.g., json).","title_case":false,"type":"str","_input_type":"StrInput"},"input_value":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"input_value","value":"","display_name":"Input","advanced":false,"input_types":["Message"],"dynamic":false,"info":"","title_case":false,"type":"str","_input_type":"MessageInput"},"metadata":{"trace_as_input":true,"list":false,"required":false,"placeholder":"","show":true,"name":"metadata","value":{},"display_name":"Metadata","advanced":true,"dynamic":false,"info":"Metadata to add to the run trace.","title_case":false,"type":"dict","_input_type":"DictInput"},"mirostat":{"trace_as_metadata":true,"options":["Disabled","Mirostat","Mirostat 2.0"],"combobox":false,"required":false,"placeholder":"","show":true,"name":"mirostat","value":"Disabled","display_name":"Mirostat","advanced":true,"dynamic":false,"info":"Enable/disable Mirostat sampling for controlling perplexity.","real_time_refresh":true,"title_case":false,"type":"str","_input_type":"DropdownInput"},"mirostat_eta":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"mirostat_eta","value":"","display_name":"Mirostat Eta","advanced":true,"dynamic":false,"info":"Learning rate for Mirostat algorithm. (Default: 0.1)","title_case":false,"type":"float","_input_type":"FloatInput"},"mirostat_tau":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"mirostat_tau","value":"","display_name":"Mirostat Tau","advanced":true,"dynamic":false,"info":"Controls the balance between coherence and diversity of the output. (Default: 5.0)","title_case":false,"type":"float","_input_type":"FloatInput"},"model_name":{"trace_as_metadata":true,"options":["aerok/zpoint_large_embedding_zh:latest","gemma2:latest"],"combobox":false,"required":false,"placeholder":"","show":true,"name":"model_name","value":"gemma2:latest","display_name":"Model Name","advanced":false,"dynamic":false,"info":"Refer to https://ollama.com/library for more models.","refresh_button":true,"title_case":false,"type":"str","_input_type":"DropdownInput"},"num_ctx":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"num_ctx","value":"","display_name":"Context Window Size","advanced":true,"dynamic":false,"info":"Size of the context window for generating tokens. (Default: 2048)","title_case":false,"type":"int","_input_type":"IntInput"},"num_gpu":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"num_gpu","value":"","display_name":"Number of GPUs","advanced":true,"dynamic":false,"info":"Number of GPUs to use for computation. (Default: 1 on macOS, 0 to disable)","title_case":false,"type":"int","_input_type":"IntInput"},"num_thread":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"num_thread","value":"","display_name":"Number of Threads","advanced":true,"dynamic":false,"info":"Number of threads to use during computation. (Default: detected for optimal performance)","title_case":false,"type":"int","_input_type":"IntInput"},"repeat_last_n":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"repeat_last_n","value":"","display_name":"Repeat Last N","advanced":true,"dynamic":false,"info":"How far back the model looks to prevent repetition. (Default: 64, 0 = disabled, -1 = num_ctx)","title_case":false,"type":"int","_input_type":"IntInput"},"repeat_penalty":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"repeat_penalty","value":"","display_name":"Repeat Penalty","advanced":true,"dynamic":false,"info":"Penalty for repetitions in generated text. (Default: 1.1)","title_case":false,"type":"float","_input_type":"FloatInput"},"stop_tokens":{"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"stop_tokens","value":"","display_name":"Stop Tokens","advanced":true,"dynamic":false,"info":"Comma-separated list of tokens to signal the model to stop generating text.","title_case":false,"type":"str","_input_type":"StrInput"},"stream":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"stream","value":false,"display_name":"Stream","advanced":true,"dynamic":false,"info":"Stream the response from the model. Streaming works only in Chat.","title_case":false,"type":"bool","_input_type":"BoolInput"},"system":{"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"system","value":"","display_name":"System","advanced":true,"dynamic":false,"info":"System to use for generating text.","title_case":false,"type":"str","_input_type":"StrInput"},"system_message":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"system_message","value":"","display_name":"System Message","advanced":true,"input_types":["Message"],"dynamic":false,"info":"System message to pass to the model.","title_case":false,"type":"str","_input_type":"MessageTextInput"},"tags":{"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"tags","value":"","display_name":"Tags","advanced":true,"dynamic":false,"info":"Comma-separated list of tags to add to the run trace.","title_case":false,"type":"str","_input_type":"StrInput"},"temperature":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"temperature","value":0,"display_name":"Temperature","advanced":false,"dynamic":false,"info":"Controls the creativity of model responses.","title_case":false,"type":"float","_input_type":"FloatInput"},"template":{"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"template","value":"","display_name":"Template","advanced":true,"dynamic":false,"info":"Template to use for generating text.","title_case":false,"type":"str","_input_type":"StrInput"},"tfs_z":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"tfs_z","value":"","display_name":"TFS Z","advanced":true,"dynamic":false,"info":"Tail free sampling value. (Default: 1)","title_case":false,"type":"float","_input_type":"FloatInput"},"timeout":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"timeout","value":"","display_name":"Timeout","advanced":true,"dynamic":false,"info":"Timeout for the request stream.","title_case":false,"type":"int","_input_type":"IntInput"},"top_k":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"top_k","value":"","display_name":"Top K","advanced":true,"dynamic":false,"info":"Limits token selection to top K. (Default: 40)","title_case":false,"type":"int","_input_type":"IntInput"},"top_p":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"top_p","value":"","display_name":"Top P","advanced":true,"dynamic":false,"info":"Works together with top-k. (Default: 0.9)","title_case":false,"type":"float","_input_type":"FloatInput"},"verbose":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"verbose","value":true,"display_name":"Verbose","advanced":false,"dynamic":false,"info":"Whether to print out response text.","title_case":false,"type":"bool","_input_type":"BoolInput"}},"description":"Generate text using Ollama Local LLMs.","icon":"Ollama","base_classes":["LanguageModel","Message"],"display_name":"Ollama","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"text_output","display_name":"Text","method":"text_response","value":"__UNDEFINED__","cache":true,"required_inputs":["input_value","stream","system_message"]},{"types":["LanguageModel"],"selected":"LanguageModel","name":"model_output","display_name":"Language Model","method":"build_model","value":"__UNDEFINED__","cache":true,"required_inputs":["base_url","format","metadata","mirostat","mirostat_eta","mirostat_tau","model_name","num_ctx","num_gpu","num_thread","repeat_last_n","repeat_penalty","stop_tokens","system","tags","temperature","template","tfs_z","timeout","top_k","top_p","verbose"]}],"field_order":["input_value","system_message","stream","base_url","model_name","temperature","format","metadata","mirostat","mirostat_eta","mirostat_tau","num_ctx","num_gpu","num_thread","repeat_last_n","repeat_penalty","tfs_z","timeout","top_k","top_p","verbose","tags","stop_tokens","system","template","output_parser"],"beta":false,"edited":false,"metadata":{},"lf_version":"1.0.19"},"id":"OllamaModel-cVR6p"},"selected":false,"width":384,"height":667,"positionAbsolute":{"x":2462.4615710061175,"y":1013.0091288873379},"dragging":false},{"id":"Prompt-hIUjS","type":"genericNode","position":{"x":1911.6529393014025,"y":1222.866350459688},"data":{"description":"Create a prompt template with dynamic variables. If using an OutputParser, you must include {format_instructions} as an additional variable.","display_name":"Prompt","id":"Prompt-hIUjS","node":{"template":{"_type":"Component","code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langflow.base.prompts.api_utils import process_prompt_template\nfrom langflow.custom import Component\nfrom langflow.inputs.inputs import DefaultPromptField\nfrom langflow.io import Output, PromptInput\nfrom langflow.schema.message import Message\nfrom langflow.template.utils import update_template_values\n\n\nclass PromptComponent(Component):\n    display_name: str = \"Prompt\"\n    description: str = \"Create a prompt template with dynamic variables.\"\n    icon = \"prompts\"\n    trace_type = \"prompt\"\n    name = \"Prompt\"\n\n    inputs = [\n        PromptInput(name=\"template\", display_name=\"Template\"),\n    ]\n\n    outputs = [\n        Output(display_name=\"Prompt Message\", name=\"prompt\", method=\"build_prompt\"),\n    ]\n\n    async def build_prompt(\n        self,\n    ) -> Message:\n        prompt = await Message.from_template_and_variables(**self._attributes)\n        self.status = prompt.text\n        return prompt\n\n    def _update_template(self, frontend_node: dict):\n        prompt_template = frontend_node[\"template\"][\"template\"][\"value\"]\n        custom_fields = frontend_node[\"custom_fields\"]\n        frontend_node_template = frontend_node[\"template\"]\n        _ = process_prompt_template(\n            template=prompt_template,\n            name=\"template\",\n            custom_fields=custom_fields,\n            frontend_node_template=frontend_node_template,\n        )\n        return frontend_node\n\n    def post_code_processing(self, new_frontend_node: dict, current_frontend_node: dict):\n        \"\"\"\n        This function is called after the code validation is done.\n        \"\"\"\n        frontend_node = super().post_code_processing(new_frontend_node, current_frontend_node)\n        template = frontend_node[\"template\"][\"template\"][\"value\"]\n        # Kept it duplicated for backwards compatibility\n        _ = process_prompt_template(\n            template=template,\n            name=\"template\",\n            custom_fields=frontend_node[\"custom_fields\"],\n            frontend_node_template=frontend_node[\"template\"],\n        )\n        # Now that template is updated, we need to grab any values that were set in the current_frontend_node\n        # and update the frontend_node with those values\n        update_template_values(new_template=frontend_node, previous_template=current_frontend_node[\"template\"])\n        return frontend_node\n\n    def _get_fallback_input(self, **kwargs):\n        return DefaultPromptField(**kwargs)\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"template":{"trace_as_input":true,"list":false,"required":false,"placeholder":"","show":true,"name":"template","value":"Think what tables do question need? What database do these tables belong to? \nList all table that is related to question.\nFormat: \"tables: table1, table2, etc.; database: database which tables belong to\"\nNo more explanations or messages, answer as format.\n\n{table_inform}\n\nUser: {user_input}","display_name":"Template","advanced":false,"dynamic":false,"info":"","title_case":false,"type":"prompt","_input_type":"PromptInput","load_from_db":false},"user_input":{"field_type":"str","required":false,"placeholder":"","list":false,"show":true,"multiline":true,"value":"","fileTypes":[],"file_path":"","name":"user_input","display_name":"user_input","advanced":false,"input_types":["Message","Text"],"dynamic":false,"info":"","load_from_db":false,"title_case":false,"type":"str"},"table_inform":{"field_type":"str","required":false,"placeholder":"","list":false,"show":true,"multiline":true,"value":"","fileTypes":[],"file_path":"","name":"table_inform","display_name":"table_inform","advanced":false,"input_types":["Message","Text"],"dynamic":false,"info":"","load_from_db":false,"title_case":false,"type":"str"}},"description":"Create a prompt template with dynamic variables.","icon":"prompts","is_input":null,"is_output":null,"is_composition":null,"base_classes":["Message"],"name":"","display_name":"Prompt","documentation":"","custom_fields":{"template":["table_inform","user_input"]},"output_types":[],"full_path":null,"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"prompt","hidden":null,"display_name":"Prompt Message","method":"build_prompt","value":"__UNDEFINED__","cache":true,"required_inputs":null}],"field_order":["template"],"beta":false,"error":null,"edited":false,"metadata":{},"lf_version":"1.0.19"},"type":"Prompt"},"selected":false,"width":384,"height":477,"positionAbsolute":{"x":1911.6529393014025,"y":1222.866350459688},"dragging":false},{"id":"CustomComponent-e2l9m","type":"genericNode","position":{"x":3059.303368001014,"y":847.3639931066272},"data":{"type":"CustomComponent","node":{"template":{"_type":"Component","embedding":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"embedding","value":"","display_name":"Embedding","advanced":false,"input_types":["Embeddings"],"dynamic":false,"info":"","title_case":false,"type":"other","_input_type":"HandleInput"},"code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"# from langflow.field_typing import Data\nfrom langchain_community.vectorstores import FAISS\n\nfrom langflow.custom import Component\nfrom langflow.io import StrInput, MultilineInput, IntInput, HandleInput, Output\nfrom langflow.schema import Data\n\nimport os\nimport re\n\nclass CustomComponent(Component):\n    display_name = \"Custom Component\"\n    description = \"Use as a template to create your own component.\"\n    documentation: str = \"http://docs.langflow.org/components/custom\"\n    icon = \"custom_components\"\n    name = \"CustomComponent\"\n\n    inputs = [\n        MultilineInput(\n            name=\"metadata_directory\",\n            display_name=\"Metadata Directory\",\n            info=\"Path of Metadata\",\n        ),\n        MultilineInput(\n            name=\"database_inform\",\n            display_name=\"Database Information\",\n        ),\n        MultilineInput(\n            name=\"input\",\n            display_name=\"Input\",\n        ),\n        IntInput(\n            name=\"keywords_topk\",\n            display_name=\"Keywords Topk\",\n        ),\n        IntInput(\n            name=\"question_topk\",\n            display_name=\"Question Topk\",\n        ),\n        HandleInput(name=\"embedding\", display_name=\"Embedding\", input_types=[\"Embeddings\"]),\n        IntInput(\n            name=\"topK\",\n            display_name=\"topK\",\n        )\n    ]\n\n    outputs = [\n        Output(display_name=\"Output\", name=\"output\", method=\"FAISS_retriever\"),\n    ]\n    \n    def get_fields(self, lines, table_name) -> str:    \n        result = ''\n        for field in lines:\n            if ((table_name.strip()) in field.split('|')[0]):\n                result += (field.split('.')[-1]+'\\n')\n\n        return result\n\n    def FAISS_retriever(self) -> Message:\n        \n        selected_inform = self.database_inform.split(';')\n        selected_driver = selected_inform[-1].split(':')[-1].strip()\n        selected_tables = selected_inform[0].split(':')[-1]\n        \n        with open(self.metadata_directory + selected_driver + \"/col_meta.txt\", 'r', encoding='utf-8') as file:\n            lines = file.read().split('\\n')\n        \n        col_list = []\n        for table_name in selected_tables.split(','):\n            for col in lines:\n                if ((table_name.strip()) in col.split('|')[0]):\n                    col_list.append(col)\n\n        col_vectors = FAISS.from_texts(col_list, self.embedding)\n        \n        for_join_keyword_list = ['代碼', '代號', 'ID']\n\n        col_inform_list = []\n        \n        for keyword in for_join_keyword_list:\n            for result in col_vectors.similarity_search_with_score(keyword, k = self.keywords_topk):\n                col_inform_list.append(result[0].page_content)\n                print(result[0].page_content)\n\n        for result in col_vectors.similarity_search_with_score(self.input, k = self.question_topk):\n            col_inform_list.append(result[0].page_content)\n            print(result[0].page_content)\n\n        selected_columns = set(col_inform_list)\n        \n        table_inform = f'table_name: {selected_tables}\\n\\n'\n        for table_name in selected_tables.split(','):\n            table_inform += f\"{lines[0].replace('table_name', table_name.strip())}\\n\"\n            if table_name.strip() != '':\n                table_inform += self.get_fields(selected_columns, table_name) + '\\n'\n        \n        return Message(\n            text=table_inform,\n        )\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"database_inform":{"trace_as_input":true,"multiline":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"database_inform","value":"","display_name":"Database Information","advanced":false,"input_types":["Message"],"dynamic":false,"info":"","title_case":false,"type":"str","_input_type":"MultilineInput"},"input":{"trace_as_input":true,"multiline":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"input","value":"","display_name":"Input","advanced":false,"input_types":["Message"],"dynamic":false,"info":"","title_case":false,"type":"str","_input_type":"MultilineInput"},"keywords_topk":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"keywords_topk","value":5,"display_name":"Keywords Topk","advanced":false,"dynamic":false,"info":"","title_case":false,"type":"int","_input_type":"IntInput"},"metadata_directory":{"trace_as_input":true,"multiline":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"metadata_directory","value":"","display_name":"Metadata Directory","advanced":false,"input_types":["Message"],"dynamic":false,"info":"Path of Metadata","title_case":false,"type":"str","_input_type":"MultilineInput"},"question_topk":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"question_topk","value":30,"display_name":"Question Topk","advanced":false,"dynamic":false,"info":"","title_case":false,"type":"int","_input_type":"IntInput"},"topK":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"topK","value":5,"display_name":"topK","advanced":false,"dynamic":false,"info":"","title_case":false,"type":"int","_input_type":"IntInput","load_from_db":false}},"description":"Use as a template to create your own component.","icon":"custom_components","base_classes":["Message"],"display_name":"Custom Component","documentation":"http://docs.langflow.org/components/custom","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"output","display_name":"Output","method":"FAISS_retriever","value":"__UNDEFINED__","cache":true}],"field_order":["metadata_directory","database_inform","input","keywords_topk","question_topk","embedding","topK"],"beta":false,"edited":true,"metadata":{},"lf_version":"1.0.19"},"id":"CustomComponent-e2l9m"},"selected":true,"width":384,"height":783,"positionAbsolute":{"x":3059.303368001014,"y":847.3639931066272},"dragging":false},{"id":"TextInput-i4UXN","type":"genericNode","position":{"x":888.3269258485657,"y":470.95270179886427},"data":{"type":"TextInput","node":{"template":{"_type":"Component","code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langflow.base.io.text import TextComponent\nfrom langflow.io import MultilineInput, Output\nfrom langflow.schema.message import Message\n\n\nclass TextInputComponent(TextComponent):\n    display_name = \"Text Input\"\n    description = \"Get text inputs from the Playground.\"\n    icon = \"type\"\n    name = \"TextInput\"\n\n    inputs = [\n        MultilineInput(\n            name=\"input_value\",\n            display_name=\"Text\",\n            info=\"Text to be passed as input.\",\n        ),\n    ]\n    outputs = [\n        Output(display_name=\"Text\", name=\"text\", method=\"text_response\"),\n    ]\n\n    def text_response(self) -> Message:\n        return Message(\n            text=self.input_value,\n        )\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"input_value":{"trace_as_input":true,"multiline":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"input_value","value":"C:/Users/admin/Downloads/Fubon_LangChain_Project_v2.3/Fubon_LangChain_Project_v2.2/database/","display_name":"Text","advanced":false,"input_types":["Message"],"dynamic":false,"info":"Text to be passed as input.","title_case":false,"type":"str","_input_type":"MultilineInput"}},"description":"Get text inputs from the Playground.","icon":"type","base_classes":["Message"],"display_name":"Metadata Directory","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"text","display_name":"Text","method":"text_response","value":"__UNDEFINED__","cache":true}],"field_order":["input_value"],"beta":false,"edited":false,"metadata":{},"lf_version":"1.0.19"},"id":"TextInput-i4UXN"},"selected":false,"width":384,"height":289,"dragging":false,"positionAbsolute":{"x":888.3269258485657,"y":470.95270179886427}},{"id":"OllamaModel-F0wUD","type":"genericNode","position":{"x":4019.482231777514,"y":928.1237145410553},"data":{"type":"OllamaModel","node":{"template":{"_type":"Component","output_parser":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"output_parser","value":"","display_name":"Output Parser","advanced":true,"input_types":["OutputParser"],"dynamic":false,"info":"The parser to use to parse the output of the model","title_case":false,"type":"other","_input_type":"HandleInput"},"base_url":{"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"base_url","value":"http://localhost:11434","display_name":"Base URL","advanced":false,"dynamic":false,"info":"Endpoint of the Ollama API. Defaults to 'http://localhost:11434' if not specified.","title_case":false,"type":"str","_input_type":"StrInput"},"code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from typing import Any\nfrom urllib.parse import urljoin\n\nimport httpx\nfrom langchain_community.chat_models import ChatOllama\n\nfrom langflow.base.models.model import LCModelComponent\nfrom langflow.field_typing import LanguageModel\nfrom langflow.inputs.inputs import HandleInput\nfrom langflow.io import BoolInput, DictInput, DropdownInput, FloatInput, IntInput, StrInput\n\n\nclass ChatOllamaComponent(LCModelComponent):\n    display_name = \"Ollama\"\n    description = \"Generate text using Ollama Local LLMs.\"\n    icon = \"Ollama\"\n    name = \"OllamaModel\"\n\n    def update_build_config(self, build_config: dict, field_value: Any, field_name: str | None = None):\n        if field_name == \"mirostat\":\n            if field_value == \"Disabled\":\n                build_config[\"mirostat_eta\"][\"advanced\"] = True\n                build_config[\"mirostat_tau\"][\"advanced\"] = True\n                build_config[\"mirostat_eta\"][\"value\"] = None\n                build_config[\"mirostat_tau\"][\"value\"] = None\n\n            else:\n                build_config[\"mirostat_eta\"][\"advanced\"] = False\n                build_config[\"mirostat_tau\"][\"advanced\"] = False\n\n                if field_value == \"Mirostat 2.0\":\n                    build_config[\"mirostat_eta\"][\"value\"] = 0.2\n                    build_config[\"mirostat_tau\"][\"value\"] = 10\n                else:\n                    build_config[\"mirostat_eta\"][\"value\"] = 0.1\n                    build_config[\"mirostat_tau\"][\"value\"] = 5\n\n        if field_name == \"model_name\":\n            base_url_dict = build_config.get(\"base_url\", {})\n            base_url_load_from_db = base_url_dict.get(\"load_from_db\", False)\n            base_url_value = base_url_dict.get(\"value\")\n            if base_url_load_from_db:\n                base_url_value = self.variables(base_url_value, field_name)\n            elif not base_url_value:\n                base_url_value = \"http://localhost:11434\"\n            build_config[\"model_name\"][\"options\"] = self.get_model(base_url_value)\n        if field_name == \"keep_alive_flag\":\n            if field_value == \"Keep\":\n                build_config[\"keep_alive\"][\"value\"] = \"-1\"\n                build_config[\"keep_alive\"][\"advanced\"] = True\n            elif field_value == \"Immediately\":\n                build_config[\"keep_alive\"][\"value\"] = \"0\"\n                build_config[\"keep_alive\"][\"advanced\"] = True\n            else:\n                build_config[\"keep_alive\"][\"advanced\"] = False\n\n        return build_config\n\n    def get_model(self, base_url_value: str) -> list[str]:\n        try:\n            url = urljoin(base_url_value, \"/api/tags\")\n            with httpx.Client() as client:\n                response = client.get(url)\n                response.raise_for_status()\n                data = response.json()\n\n                return [model[\"name\"] for model in data.get(\"models\", [])]\n        except Exception as e:\n            msg = \"Could not retrieve models. Please, make sure Ollama is running.\"\n            raise ValueError(msg) from e\n\n    inputs = [\n        *LCModelComponent._base_inputs,\n        StrInput(\n            name=\"base_url\",\n            display_name=\"Base URL\",\n            info=\"Endpoint of the Ollama API. Defaults to 'http://localhost:11434' if not specified.\",\n            value=\"http://localhost:11434\",\n        ),\n        DropdownInput(\n            name=\"model_name\",\n            display_name=\"Model Name\",\n            value=\"llama3.1\",\n            info=\"Refer to https://ollama.com/library for more models.\",\n            refresh_button=True,\n        ),\n        FloatInput(\n            name=\"temperature\",\n            display_name=\"Temperature\",\n            value=0.2,\n            info=\"Controls the creativity of model responses.\",\n        ),\n        StrInput(\n            name=\"format\", display_name=\"Format\", info=\"Specify the format of the output (e.g., json).\", advanced=True\n        ),\n        DictInput(name=\"metadata\", display_name=\"Metadata\", info=\"Metadata to add to the run trace.\", advanced=True),\n        DropdownInput(\n            name=\"mirostat\",\n            display_name=\"Mirostat\",\n            options=[\"Disabled\", \"Mirostat\", \"Mirostat 2.0\"],\n            info=\"Enable/disable Mirostat sampling for controlling perplexity.\",\n            value=\"Disabled\",\n            advanced=True,\n            real_time_refresh=True,\n        ),\n        FloatInput(\n            name=\"mirostat_eta\",\n            display_name=\"Mirostat Eta\",\n            info=\"Learning rate for Mirostat algorithm. (Default: 0.1)\",\n            advanced=True,\n        ),\n        FloatInput(\n            name=\"mirostat_tau\",\n            display_name=\"Mirostat Tau\",\n            info=\"Controls the balance between coherence and diversity of the output. (Default: 5.0)\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"num_ctx\",\n            display_name=\"Context Window Size\",\n            info=\"Size of the context window for generating tokens. (Default: 2048)\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"num_gpu\",\n            display_name=\"Number of GPUs\",\n            info=\"Number of GPUs to use for computation. (Default: 1 on macOS, 0 to disable)\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"num_thread\",\n            display_name=\"Number of Threads\",\n            info=\"Number of threads to use during computation. (Default: detected for optimal performance)\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"repeat_last_n\",\n            display_name=\"Repeat Last N\",\n            info=\"How far back the model looks to prevent repetition. (Default: 64, 0 = disabled, -1 = num_ctx)\",\n            advanced=True,\n        ),\n        FloatInput(\n            name=\"repeat_penalty\",\n            display_name=\"Repeat Penalty\",\n            info=\"Penalty for repetitions in generated text. (Default: 1.1)\",\n            advanced=True,\n        ),\n        FloatInput(name=\"tfs_z\", display_name=\"TFS Z\", info=\"Tail free sampling value. (Default: 1)\", advanced=True),\n        IntInput(name=\"timeout\", display_name=\"Timeout\", info=\"Timeout for the request stream.\", advanced=True),\n        IntInput(\n            name=\"top_k\", display_name=\"Top K\", info=\"Limits token selection to top K. (Default: 40)\", advanced=True\n        ),\n        FloatInput(name=\"top_p\", display_name=\"Top P\", info=\"Works together with top-k. (Default: 0.9)\", advanced=True),\n        BoolInput(name=\"verbose\", display_name=\"Verbose\", info=\"Whether to print out response text.\"),\n        StrInput(\n            name=\"tags\",\n            display_name=\"Tags\",\n            info=\"Comma-separated list of tags to add to the run trace.\",\n            advanced=True,\n        ),\n        StrInput(\n            name=\"stop_tokens\",\n            display_name=\"Stop Tokens\",\n            info=\"Comma-separated list of tokens to signal the model to stop generating text.\",\n            advanced=True,\n        ),\n        StrInput(name=\"system\", display_name=\"System\", info=\"System to use for generating text.\", advanced=True),\n        StrInput(name=\"template\", display_name=\"Template\", info=\"Template to use for generating text.\", advanced=True),\n        HandleInput(\n            name=\"output_parser\",\n            display_name=\"Output Parser\",\n            info=\"The parser to use to parse the output of the model\",\n            advanced=True,\n            input_types=[\"OutputParser\"],\n        ),\n    ]\n\n    def build_model(self) -> LanguageModel:  # type: ignore[type-var]\n        # Mapping mirostat settings to their corresponding values\n        mirostat_options = {\"Mirostat\": 1, \"Mirostat 2.0\": 2}\n\n        # Default to 0 for 'Disabled'\n        mirostat_value = mirostat_options.get(self.mirostat, 0)\n\n        # Set mirostat_eta and mirostat_tau to None if mirostat is disabled\n        if mirostat_value == 0:\n            mirostat_eta = None\n            mirostat_tau = None\n        else:\n            mirostat_eta = self.mirostat_eta\n            mirostat_tau = self.mirostat_tau\n\n        # Mapping system settings to their corresponding values\n        llm_params = {\n            \"base_url\": self.base_url,\n            \"model\": self.model_name,\n            \"mirostat\": mirostat_value,\n            \"format\": self.format,\n            \"metadata\": self.metadata,\n            \"tags\": self.tags.split(\",\") if self.tags else None,\n            \"mirostat_eta\": mirostat_eta,\n            \"mirostat_tau\": mirostat_tau,\n            \"num_ctx\": self.num_ctx or None,\n            \"num_gpu\": self.num_gpu or None,\n            \"num_thread\": self.num_thread or None,\n            \"repeat_last_n\": self.repeat_last_n or None,\n            \"repeat_penalty\": self.repeat_penalty or None,\n            \"temperature\": self.temperature or None,\n            \"stop\": self.stop_tokens.split(\",\") if self.stop_tokens else None,\n            \"system\": self.system,\n            \"template\": self.template,\n            \"tfs_z\": self.tfs_z or None,\n            \"timeout\": self.timeout or None,\n            \"top_k\": self.top_k or None,\n            \"top_p\": self.top_p or None,\n            \"verbose\": self.verbose,\n        }\n\n        # Remove parameters with None values\n        llm_params = {k: v for k, v in llm_params.items() if v is not None}\n\n        try:\n            output = ChatOllama(**llm_params)\n        except Exception as e:\n            msg = \"Could not initialize Ollama LLM.\"\n            raise ValueError(msg) from e\n\n        return output\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"format":{"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"format","value":"","display_name":"Format","advanced":true,"dynamic":false,"info":"Specify the format of the output (e.g., json).","title_case":false,"type":"str","_input_type":"StrInput"},"input_value":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"input_value","value":"","display_name":"Input","advanced":false,"input_types":["Message"],"dynamic":false,"info":"","title_case":false,"type":"str","_input_type":"MessageInput"},"metadata":{"trace_as_input":true,"list":false,"required":false,"placeholder":"","show":true,"name":"metadata","value":{},"display_name":"Metadata","advanced":true,"dynamic":false,"info":"Metadata to add to the run trace.","title_case":false,"type":"dict","_input_type":"DictInput"},"mirostat":{"trace_as_metadata":true,"options":["Disabled","Mirostat","Mirostat 2.0"],"combobox":false,"required":false,"placeholder":"","show":true,"name":"mirostat","value":"Disabled","display_name":"Mirostat","advanced":true,"dynamic":false,"info":"Enable/disable Mirostat sampling for controlling perplexity.","real_time_refresh":true,"title_case":false,"type":"str","_input_type":"DropdownInput"},"mirostat_eta":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"mirostat_eta","value":"","display_name":"Mirostat Eta","advanced":true,"dynamic":false,"info":"Learning rate for Mirostat algorithm. (Default: 0.1)","title_case":false,"type":"float","_input_type":"FloatInput"},"mirostat_tau":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"mirostat_tau","value":"","display_name":"Mirostat Tau","advanced":true,"dynamic":false,"info":"Controls the balance between coherence and diversity of the output. (Default: 5.0)","title_case":false,"type":"float","_input_type":"FloatInput"},"model_name":{"trace_as_metadata":true,"options":["aerok/zpoint_large_embedding_zh:latest","gemma2:latest"],"combobox":false,"required":false,"placeholder":"","show":true,"name":"model_name","value":"gemma2:latest","display_name":"Model Name","advanced":false,"dynamic":false,"info":"Refer to https://ollama.com/library for more models.","refresh_button":true,"title_case":false,"type":"str","_input_type":"DropdownInput"},"num_ctx":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"num_ctx","value":"","display_name":"Context Window Size","advanced":true,"dynamic":false,"info":"Size of the context window for generating tokens. (Default: 2048)","title_case":false,"type":"int","_input_type":"IntInput"},"num_gpu":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"num_gpu","value":"","display_name":"Number of GPUs","advanced":true,"dynamic":false,"info":"Number of GPUs to use for computation. (Default: 1 on macOS, 0 to disable)","title_case":false,"type":"int","_input_type":"IntInput"},"num_thread":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"num_thread","value":"","display_name":"Number of Threads","advanced":true,"dynamic":false,"info":"Number of threads to use during computation. (Default: detected for optimal performance)","title_case":false,"type":"int","_input_type":"IntInput"},"repeat_last_n":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"repeat_last_n","value":"","display_name":"Repeat Last N","advanced":true,"dynamic":false,"info":"How far back the model looks to prevent repetition. (Default: 64, 0 = disabled, -1 = num_ctx)","title_case":false,"type":"int","_input_type":"IntInput"},"repeat_penalty":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"repeat_penalty","value":"","display_name":"Repeat Penalty","advanced":true,"dynamic":false,"info":"Penalty for repetitions in generated text. (Default: 1.1)","title_case":false,"type":"float","_input_type":"FloatInput"},"stop_tokens":{"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"stop_tokens","value":"","display_name":"Stop Tokens","advanced":true,"dynamic":false,"info":"Comma-separated list of tokens to signal the model to stop generating text.","title_case":false,"type":"str","_input_type":"StrInput"},"stream":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"stream","value":false,"display_name":"Stream","advanced":true,"dynamic":false,"info":"Stream the response from the model. Streaming works only in Chat.","title_case":false,"type":"bool","_input_type":"BoolInput"},"system":{"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"system","value":"","display_name":"System","advanced":true,"dynamic":false,"info":"System to use for generating text.","title_case":false,"type":"str","_input_type":"StrInput"},"system_message":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"system_message","value":"","display_name":"System Message","advanced":true,"input_types":["Message"],"dynamic":false,"info":"System message to pass to the model.","title_case":false,"type":"str","_input_type":"MessageTextInput"},"tags":{"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"tags","value":"","display_name":"Tags","advanced":true,"dynamic":false,"info":"Comma-separated list of tags to add to the run trace.","title_case":false,"type":"str","_input_type":"StrInput"},"temperature":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"temperature","value":0,"display_name":"Temperature","advanced":false,"dynamic":false,"info":"Controls the creativity of model responses.","title_case":false,"type":"float","_input_type":"FloatInput"},"template":{"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"template","value":"","display_name":"Template","advanced":true,"dynamic":false,"info":"Template to use for generating text.","title_case":false,"type":"str","_input_type":"StrInput"},"tfs_z":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"tfs_z","value":"","display_name":"TFS Z","advanced":true,"dynamic":false,"info":"Tail free sampling value. (Default: 1)","title_case":false,"type":"float","_input_type":"FloatInput"},"timeout":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"timeout","value":"","display_name":"Timeout","advanced":true,"dynamic":false,"info":"Timeout for the request stream.","title_case":false,"type":"int","_input_type":"IntInput"},"top_k":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"top_k","value":"","display_name":"Top K","advanced":true,"dynamic":false,"info":"Limits token selection to top K. (Default: 40)","title_case":false,"type":"int","_input_type":"IntInput"},"top_p":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"top_p","value":"","display_name":"Top P","advanced":true,"dynamic":false,"info":"Works together with top-k. (Default: 0.9)","title_case":false,"type":"float","_input_type":"FloatInput"},"verbose":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"verbose","value":true,"display_name":"Verbose","advanced":false,"dynamic":false,"info":"Whether to print out response text.","title_case":false,"type":"bool","_input_type":"BoolInput"}},"description":"Generate text using Ollama Local LLMs.","icon":"Ollama","base_classes":["LanguageModel","Message"],"display_name":"Ollama","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"text_output","display_name":"Text","method":"text_response","value":"__UNDEFINED__","cache":true,"required_inputs":["input_value","stream","system_message"]},{"types":["LanguageModel"],"selected":"LanguageModel","name":"model_output","display_name":"Language Model","method":"build_model","value":"__UNDEFINED__","cache":true,"required_inputs":["base_url","format","metadata","mirostat","mirostat_eta","mirostat_tau","model_name","num_ctx","num_gpu","num_thread","repeat_last_n","repeat_penalty","stop_tokens","system","tags","temperature","template","tfs_z","timeout","top_k","top_p","verbose"]}],"field_order":["input_value","system_message","stream","base_url","model_name","temperature","format","metadata","mirostat","mirostat_eta","mirostat_tau","num_ctx","num_gpu","num_thread","repeat_last_n","repeat_penalty","tfs_z","timeout","top_k","top_p","verbose","tags","stop_tokens","system","template","output_parser"],"beta":false,"edited":false,"metadata":{},"lf_version":"1.0.19"},"id":"OllamaModel-F0wUD"},"selected":false,"width":384,"height":667,"positionAbsolute":{"x":4019.482231777514,"y":928.1237145410553},"dragging":false},{"id":"Prompt-arpwd","type":"genericNode","position":{"x":3539.0554346560657,"y":1088.8680188410885},"data":{"description":"Create a prompt template with dynamic variables. If using an OutputParser, you must include {format_instructions} as an additional variable.","display_name":"Prompt","id":"Prompt-arpwd","node":{"template":{"_type":"Component","code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langflow.base.prompts.api_utils import process_prompt_template\nfrom langflow.custom import Component\nfrom langflow.inputs.inputs import DefaultPromptField\nfrom langflow.io import Output, PromptInput\nfrom langflow.schema.message import Message\nfrom langflow.template.utils import update_template_values\n\n\nclass PromptComponent(Component):\n    display_name: str = \"Prompt\"\n    description: str = \"Create a prompt template with dynamic variables.\"\n    icon = \"prompts\"\n    trace_type = \"prompt\"\n    name = \"Prompt\"\n\n    inputs = [\n        PromptInput(name=\"template\", display_name=\"Template\"),\n    ]\n\n    outputs = [\n        Output(display_name=\"Prompt Message\", name=\"prompt\", method=\"build_prompt\"),\n    ]\n\n    async def build_prompt(\n        self,\n    ) -> Message:\n        prompt = await Message.from_template_and_variables(**self._attributes)\n        self.status = prompt.text\n        return prompt\n\n    def _update_template(self, frontend_node: dict):\n        prompt_template = frontend_node[\"template\"][\"template\"][\"value\"]\n        custom_fields = frontend_node[\"custom_fields\"]\n        frontend_node_template = frontend_node[\"template\"]\n        _ = process_prompt_template(\n            template=prompt_template,\n            name=\"template\",\n            custom_fields=custom_fields,\n            frontend_node_template=frontend_node_template,\n        )\n        return frontend_node\n\n    def post_code_processing(self, new_frontend_node: dict, current_frontend_node: dict):\n        \"\"\"\n        This function is called after the code validation is done.\n        \"\"\"\n        frontend_node = super().post_code_processing(new_frontend_node, current_frontend_node)\n        template = frontend_node[\"template\"][\"template\"][\"value\"]\n        # Kept it duplicated for backwards compatibility\n        _ = process_prompt_template(\n            template=template,\n            name=\"template\",\n            custom_fields=frontend_node[\"custom_fields\"],\n            frontend_node_template=frontend_node[\"template\"],\n        )\n        # Now that template is updated, we need to grab any values that were set in the current_frontend_node\n        # and update the frontend_node with those values\n        update_template_values(new_template=frontend_node, previous_template=current_frontend_node[\"template\"])\n        return frontend_node\n\n    def _get_fallback_input(self, **kwargs):\n        return DefaultPromptField(**kwargs)\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"template":{"trace_as_input":true,"list":false,"required":false,"placeholder":"","show":true,"name":"template","value":"{table_inform}\n\nFirst, follow the example:\n1. 問題中有與'總數'、'總'相關字詞： SELECT SUM() FROM table_name GROUP BY\n2. 問題中有與'筆數'相關字詞: SELECT COUNT(*) GROUP BY\n3. 問題中有與日期'年'、'月'相關字詞： YEAR() or MONTH()\n4. 問題中有與'最'、'第n高'相關字詞： ORDER BY column_name (DESC or ASC) LIMIT 1 (OFFSET n-1);\n5. 字串匹配優先使用： WHERE column_name LIKE '%STRING%'\n\nThen, follow the rules:\n1. use HAVING aggregates condition with columns in GROUP BY instead of WHERE.\n2. Don't use code number matching name as condition, code number is not human name.\n3. JOIN every table.\n\nFinally, answer as format \n```sql\n   SELECT (DISTINCT) column_name FROM table_name;\n```, choose required columns depends on metadata.\n\nUser: \n{user_input}\n{keywords}","display_name":"Template","advanced":false,"dynamic":false,"info":"","title_case":false,"type":"prompt","_input_type":"PromptInput","load_from_db":false},"user_input":{"field_type":"str","required":false,"placeholder":"","list":false,"show":true,"multiline":true,"value":"","fileTypes":[],"file_path":"","name":"user_input","display_name":"user_input","advanced":false,"input_types":["Message","Text"],"dynamic":false,"info":"","load_from_db":false,"title_case":false,"type":"str"},"table_inform":{"field_type":"str","required":false,"placeholder":"","list":false,"show":true,"multiline":true,"value":"","fileTypes":[],"file_path":"","name":"table_inform","display_name":"table_inform","advanced":false,"input_types":["Message","Text"],"dynamic":false,"info":"","load_from_db":false,"title_case":false,"type":"str"},"keywords":{"field_type":"str","required":false,"placeholder":"","list":false,"show":true,"multiline":true,"value":"","fileTypes":[],"file_path":"","name":"keywords","display_name":"keywords","advanced":false,"input_types":["Message","Text"],"dynamic":false,"info":"","load_from_db":false,"title_case":false,"type":"str"}},"description":"Create a prompt template with dynamic variables.","icon":"prompts","is_input":null,"is_output":null,"is_composition":null,"base_classes":["Message"],"name":"","display_name":"Prompt","documentation":"","custom_fields":{"template":["table_inform","user_input","keywords"]},"output_types":[],"full_path":null,"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"prompt","hidden":null,"display_name":"Prompt Message","method":"build_prompt","value":"__UNDEFINED__","cache":true,"required_inputs":null}],"field_order":["template"],"beta":false,"error":null,"edited":false,"metadata":{},"lf_version":"1.0.19"},"type":"Prompt"},"selected":false,"width":384,"height":563,"positionAbsolute":{"x":3539.0554346560657,"y":1088.8680188410885},"dragging":false}],"edges":[{"source":"ChatInput-jV0J9","sourceHandle":"{œdataTypeœ:œChatInputœ,œidœ:œChatInput-jV0J9œ,œnameœ:œmessageœ,œoutput_typesœ:[œMessageœ]}","target":"Prompt-lGhTS","targetHandle":"{œfieldNameœ:œuser_inputœ,œidœ:œPrompt-lGhTSœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"user_input","id":"Prompt-lGhTS","inputTypes":["Message","Text"],"type":"str"},"sourceHandle":{"dataType":"ChatInput","id":"ChatInput-jV0J9","name":"message","output_types":["Message"]}},"id":"reactflow__edge-ChatInput-jV0J9{œdataTypeœ:œChatInputœ,œidœ:œChatInput-jV0J9œ,œnameœ:œmessageœ,œoutput_typesœ:[œMessageœ]}-Prompt-lGhTS{œfieldNameœ:œuser_inputœ,œidœ:œPrompt-lGhTSœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}","animated":false,"className":""},{"source":"Prompt-lGhTS","sourceHandle":"{œdataTypeœ:œPromptœ,œidœ:œPrompt-lGhTSœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}","target":"OllamaModel-kGRvk","targetHandle":"{œfieldNameœ:œinput_valueœ,œidœ:œOllamaModel-kGRvkœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"input_value","id":"OllamaModel-kGRvk","inputTypes":["Message"],"type":"str"},"sourceHandle":{"dataType":"Prompt","id":"Prompt-lGhTS","name":"prompt","output_types":["Message"]}},"id":"reactflow__edge-Prompt-lGhTS{œdataTypeœ:œPromptœ,œidœ:œPrompt-lGhTSœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}-OllamaModel-kGRvk{œfieldNameœ:œinput_valueœ,œidœ:œOllamaModel-kGRvkœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","animated":false,"className":""},{"source":"OllamaEmbeddings-uJKV7","sourceHandle":"{œdataTypeœ:œOllamaEmbeddingsœ,œidœ:œOllamaEmbeddings-uJKV7œ,œnameœ:œembeddingsœ,œoutput_typesœ:[œEmbeddingsœ]}","target":"CustomComponent-RFa62","targetHandle":"{œfieldNameœ:œembeddingœ,œidœ:œCustomComponent-RFa62œ,œinputTypesœ:[œEmbeddingsœ],œtypeœ:œotherœ}","data":{"targetHandle":{"fieldName":"embedding","id":"CustomComponent-RFa62","inputTypes":["Embeddings"],"type":"other"},"sourceHandle":{"dataType":"OllamaEmbeddings","id":"OllamaEmbeddings-uJKV7","name":"embeddings","output_types":["Embeddings"]}},"id":"reactflow__edge-OllamaEmbeddings-uJKV7{œdataTypeœ:œOllamaEmbeddingsœ,œidœ:œOllamaEmbeddings-uJKV7œ,œnameœ:œembeddingsœ,œoutput_typesœ:[œEmbeddingsœ]}-CustomComponent-RFa62{œfieldNameœ:œembeddingœ,œidœ:œCustomComponent-RFa62œ,œinputTypesœ:[œEmbeddingsœ],œtypeœ:œotherœ}","animated":false,"className":""},{"source":"OllamaModel-kGRvk","sourceHandle":"{œdataTypeœ:œOllamaModelœ,œidœ:œOllamaModel-kGRvkœ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}","target":"CustomComponent-RFa62","targetHandle":"{œfieldNameœ:œkeywordsœ,œidœ:œCustomComponent-RFa62œ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"keywords","id":"CustomComponent-RFa62","inputTypes":["Message"],"type":"str"},"sourceHandle":{"dataType":"OllamaModel","id":"OllamaModel-kGRvk","name":"text_output","output_types":["Message"]}},"id":"reactflow__edge-OllamaModel-kGRvk{œdataTypeœ:œOllamaModelœ,œidœ:œOllamaModel-kGRvkœ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}-CustomComponent-RFa62{œfieldNameœ:œkeywordsœ,œidœ:œCustomComponent-RFa62œ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","animated":false,"className":""},{"source":"CustomComponent-RFa62","sourceHandle":"{œdataTypeœ:œCustomComponentœ,œidœ:œCustomComponent-RFa62œ,œnameœ:œoutputœ,œoutput_typesœ:[œMessageœ]}","target":"Prompt-hIUjS","targetHandle":"{œfieldNameœ:œtable_informœ,œidœ:œPrompt-hIUjSœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"table_inform","id":"Prompt-hIUjS","inputTypes":["Message","Text"],"type":"str"},"sourceHandle":{"dataType":"CustomComponent","id":"CustomComponent-RFa62","name":"output","output_types":["Message"]}},"id":"reactflow__edge-CustomComponent-RFa62{œdataTypeœ:œCustomComponentœ,œidœ:œCustomComponent-RFa62œ,œnameœ:œoutputœ,œoutput_typesœ:[œMessageœ]}-Prompt-hIUjS{œfieldNameœ:œtable_informœ,œidœ:œPrompt-hIUjSœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}","animated":false,"className":""},{"source":"ChatInput-jV0J9","sourceHandle":"{œdataTypeœ:œChatInputœ,œidœ:œChatInput-jV0J9œ,œnameœ:œmessageœ,œoutput_typesœ:[œMessageœ]}","target":"Prompt-hIUjS","targetHandle":"{œfieldNameœ:œuser_inputœ,œidœ:œPrompt-hIUjSœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"user_input","id":"Prompt-hIUjS","inputTypes":["Message","Text"],"type":"str"},"sourceHandle":{"dataType":"ChatInput","id":"ChatInput-jV0J9","name":"message","output_types":["Message"]}},"id":"reactflow__edge-ChatInput-jV0J9{œdataTypeœ:œChatInputœ,œidœ:œChatInput-jV0J9œ,œnameœ:œmessageœ,œoutput_typesœ:[œMessageœ]}-Prompt-hIUjS{œfieldNameœ:œuser_inputœ,œidœ:œPrompt-hIUjSœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}","animated":false,"className":""},{"source":"Prompt-hIUjS","sourceHandle":"{œdataTypeœ:œPromptœ,œidœ:œPrompt-hIUjSœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}","target":"OllamaModel-cVR6p","targetHandle":"{œfieldNameœ:œinput_valueœ,œidœ:œOllamaModel-cVR6pœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"input_value","id":"OllamaModel-cVR6p","inputTypes":["Message"],"type":"str"},"sourceHandle":{"dataType":"Prompt","id":"Prompt-hIUjS","name":"prompt","output_types":["Message"]}},"id":"reactflow__edge-Prompt-hIUjS{œdataTypeœ:œPromptœ,œidœ:œPrompt-hIUjSœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}-OllamaModel-cVR6p{œfieldNameœ:œinput_valueœ,œidœ:œOllamaModel-cVR6pœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","animated":false,"className":""},{"source":"TextInput-i4UXN","sourceHandle":"{œdataTypeœ:œTextInputœ,œidœ:œTextInput-i4UXNœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}","target":"CustomComponent-e2l9m","targetHandle":"{œfieldNameœ:œmetadata_directoryœ,œidœ:œCustomComponent-e2l9mœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"metadata_directory","id":"CustomComponent-e2l9m","inputTypes":["Message"],"type":"str"},"sourceHandle":{"dataType":"TextInput","id":"TextInput-i4UXN","name":"text","output_types":["Message"]}},"id":"reactflow__edge-TextInput-i4UXN{œdataTypeœ:œTextInputœ,œidœ:œTextInput-i4UXNœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}-CustomComponent-e2l9m{œfieldNameœ:œmetadata_directoryœ,œidœ:œCustomComponent-e2l9mœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","animated":false,"className":""},{"source":"TextInput-i4UXN","sourceHandle":"{œdataTypeœ:œTextInputœ,œidœ:œTextInput-i4UXNœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}","target":"CustomComponent-RFa62","targetHandle":"{œfieldNameœ:œmetadata_directoryœ,œidœ:œCustomComponent-RFa62œ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"metadata_directory","id":"CustomComponent-RFa62","inputTypes":["Message"],"type":"str"},"sourceHandle":{"dataType":"TextInput","id":"TextInput-i4UXN","name":"text","output_types":["Message"]}},"id":"reactflow__edge-TextInput-i4UXN{œdataTypeœ:œTextInputœ,œidœ:œTextInput-i4UXNœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}-CustomComponent-RFa62{œfieldNameœ:œmetadata_directoryœ,œidœ:œCustomComponent-RFa62œ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","animated":false,"className":""},{"source":"OllamaEmbeddings-uJKV7","sourceHandle":"{œdataTypeœ:œOllamaEmbeddingsœ,œidœ:œOllamaEmbeddings-uJKV7œ,œnameœ:œembeddingsœ,œoutput_typesœ:[œEmbeddingsœ]}","target":"CustomComponent-e2l9m","targetHandle":"{œfieldNameœ:œembeddingœ,œidœ:œCustomComponent-e2l9mœ,œinputTypesœ:[œEmbeddingsœ],œtypeœ:œotherœ}","data":{"targetHandle":{"fieldName":"embedding","id":"CustomComponent-e2l9m","inputTypes":["Embeddings"],"type":"other"},"sourceHandle":{"dataType":"OllamaEmbeddings","id":"OllamaEmbeddings-uJKV7","name":"embeddings","output_types":["Embeddings"]}},"id":"reactflow__edge-OllamaEmbeddings-uJKV7{œdataTypeœ:œOllamaEmbeddingsœ,œidœ:œOllamaEmbeddings-uJKV7œ,œnameœ:œembeddingsœ,œoutput_typesœ:[œEmbeddingsœ]}-CustomComponent-e2l9m{œfieldNameœ:œembeddingœ,œidœ:œCustomComponent-e2l9mœ,œinputTypesœ:[œEmbeddingsœ],œtypeœ:œotherœ}","animated":false,"className":""},{"source":"OllamaModel-cVR6p","sourceHandle":"{œdataTypeœ:œOllamaModelœ,œidœ:œOllamaModel-cVR6pœ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}","target":"CustomComponent-e2l9m","targetHandle":"{œfieldNameœ:œdatabase_informœ,œidœ:œCustomComponent-e2l9mœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"database_inform","id":"CustomComponent-e2l9m","inputTypes":["Message"],"type":"str"},"sourceHandle":{"dataType":"OllamaModel","id":"OllamaModel-cVR6p","name":"text_output","output_types":["Message"]}},"id":"reactflow__edge-OllamaModel-cVR6p{œdataTypeœ:œOllamaModelœ,œidœ:œOllamaModel-cVR6pœ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}-CustomComponent-e2l9m{œfieldNameœ:œdatabase_informœ,œidœ:œCustomComponent-e2l9mœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","animated":false,"className":""},{"source":"ChatInput-jV0J9","sourceHandle":"{œdataTypeœ:œChatInputœ,œidœ:œChatInput-jV0J9œ,œnameœ:œmessageœ,œoutput_typesœ:[œMessageœ]}","target":"CustomComponent-e2l9m","targetHandle":"{œfieldNameœ:œinputœ,œidœ:œCustomComponent-e2l9mœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"input","id":"CustomComponent-e2l9m","inputTypes":["Message"],"type":"str"},"sourceHandle":{"dataType":"ChatInput","id":"ChatInput-jV0J9","name":"message","output_types":["Message"]}},"id":"reactflow__edge-ChatInput-jV0J9{œdataTypeœ:œChatInputœ,œidœ:œChatInput-jV0J9œ,œnameœ:œmessageœ,œoutput_typesœ:[œMessageœ]}-CustomComponent-e2l9m{œfieldNameœ:œinputœ,œidœ:œCustomComponent-e2l9mœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","animated":false,"className":""},{"source":"OllamaModel-kGRvk","sourceHandle":"{œdataTypeœ:œOllamaModelœ,œidœ:œOllamaModel-kGRvkœ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}","target":"Prompt-arpwd","targetHandle":"{œfieldNameœ:œkeywordsœ,œidœ:œPrompt-arpwdœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"keywords","id":"Prompt-arpwd","inputTypes":["Message","Text"],"type":"str"},"sourceHandle":{"dataType":"OllamaModel","id":"OllamaModel-kGRvk","name":"text_output","output_types":["Message"]}},"id":"reactflow__edge-OllamaModel-kGRvk{œdataTypeœ:œOllamaModelœ,œidœ:œOllamaModel-kGRvkœ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}-Prompt-arpwd{œfieldNameœ:œkeywordsœ,œidœ:œPrompt-arpwdœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}","animated":false,"className":""},{"source":"ChatInput-jV0J9","sourceHandle":"{œdataTypeœ:œChatInputœ,œidœ:œChatInput-jV0J9œ,œnameœ:œmessageœ,œoutput_typesœ:[œMessageœ]}","target":"Prompt-arpwd","targetHandle":"{œfieldNameœ:œuser_inputœ,œidœ:œPrompt-arpwdœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"user_input","id":"Prompt-arpwd","inputTypes":["Message","Text"],"type":"str"},"sourceHandle":{"dataType":"ChatInput","id":"ChatInput-jV0J9","name":"message","output_types":["Message"]}},"id":"reactflow__edge-ChatInput-jV0J9{œdataTypeœ:œChatInputœ,œidœ:œChatInput-jV0J9œ,œnameœ:œmessageœ,œoutput_typesœ:[œMessageœ]}-Prompt-arpwd{œfieldNameœ:œuser_inputœ,œidœ:œPrompt-arpwdœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}","animated":false,"className":""},{"source":"CustomComponent-e2l9m","sourceHandle":"{œdataTypeœ:œCustomComponentœ,œidœ:œCustomComponent-e2l9mœ,œnameœ:œoutputœ,œoutput_typesœ:[œMessageœ]}","target":"Prompt-arpwd","targetHandle":"{œfieldNameœ:œtable_informœ,œidœ:œPrompt-arpwdœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"table_inform","id":"Prompt-arpwd","inputTypes":["Message","Text"],"type":"str"},"sourceHandle":{"dataType":"CustomComponent","id":"CustomComponent-e2l9m","name":"output","output_types":["Message"]}},"id":"reactflow__edge-CustomComponent-e2l9m{œdataTypeœ:œCustomComponentœ,œidœ:œCustomComponent-e2l9mœ,œnameœ:œoutputœ,œoutput_typesœ:[œMessageœ]}-Prompt-arpwd{œfieldNameœ:œtable_informœ,œidœ:œPrompt-arpwdœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}","animated":false,"className":""},{"source":"Prompt-arpwd","sourceHandle":"{œdataTypeœ:œPromptœ,œidœ:œPrompt-arpwdœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}","target":"OllamaModel-F0wUD","targetHandle":"{œfieldNameœ:œinput_valueœ,œidœ:œOllamaModel-F0wUDœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"input_value","id":"OllamaModel-F0wUD","inputTypes":["Message"],"type":"str"},"sourceHandle":{"dataType":"Prompt","id":"Prompt-arpwd","name":"prompt","output_types":["Message"]}},"id":"reactflow__edge-Prompt-arpwd{œdataTypeœ:œPromptœ,œidœ:œPrompt-arpwdœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}-OllamaModel-F0wUD{œfieldNameœ:œinput_valueœ,œidœ:œOllamaModel-F0wUDœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","animated":false,"className":""},{"source":"OllamaModel-F0wUD","sourceHandle":"{œdataTypeœ:œOllamaModelœ,œidœ:œOllamaModel-F0wUDœ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}","target":"ChatOutput-zJS5p","targetHandle":"{œfieldNameœ:œinput_valueœ,œidœ:œChatOutput-zJS5pœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"input_value","id":"ChatOutput-zJS5p","inputTypes":["Message"],"type":"str"},"sourceHandle":{"dataType":"OllamaModel","id":"OllamaModel-F0wUD","name":"text_output","output_types":["Message"]}},"id":"reactflow__edge-OllamaModel-F0wUD{œdataTypeœ:œOllamaModelœ,œidœ:œOllamaModel-F0wUDœ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}-ChatOutput-zJS5p{œfieldNameœ:œinput_valueœ,œidœ:œChatOutput-zJS5pœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","animated":false,"className":""}],"viewport":{"x":-1214.9283077091186,"y":-262.4823991222205,"zoom":0.532463924894364}},"description":"This flow will get you experimenting with the basics of the UI, the Chat and the Prompt component. \n\nTry changing the Template in it to see how the model behaves. \nYou can change it to this and a Text Input into the `type_of_person` variable : \"Answer the user as if you were a pirate.\n\nUser: {user_input}\n\nAnswer: \" ","name":"SQL_LLM","last_tested_version":"1.0.19","endpoint_name":null,"is_component":false}